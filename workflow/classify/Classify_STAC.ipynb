{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b87702-8cb7-47d4-924b-8fe0861cd179",
   "metadata": {},
   "source": [
    "# **Classifying Sentinel-2 data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d67ea9-4fd0-4a6c-95c6-66672b6bdc6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification.py\n",
    "\"\"\"\n",
    "Machnine learning classification tools for analysing remote sensing data\n",
    "using the Open Data Cube.\n",
    "\n",
    "License: The code in this notebook is licensed under the Apache License,\n",
    "Version 2.0 (https://www.apache.org/licenses/LICENSE-2.0). Digital Earth\n",
    "Australia data is licensed under the Creative Commons by Attribution 4.0\n",
    "license (https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Contact: If you need assistance, please post a question on the Open Data\n",
    "Cube Slack channel (http://slack.opendatacube.org/) or on the GIS Stack\n",
    "Exchange (https://gis.stackexchange.com/questions/ask?tags=open-data-cube)\n",
    "using the `open-data-cube` tag (you can view previously asked questions\n",
    "here: https://gis.stackexchange.com/questions/tagged/open-data-cube).\n",
    "\n",
    "If you would like to report an issue with this script, you can file one\n",
    "on Github (https://github.com/GeoscienceAustralia/dea-notebooks/issues/new).\n",
    "\n",
    "Last modified: May 2021\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import datacube\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import dask.array as da\n",
    "import geopandas as gpd\n",
    "from copy import deepcopy\n",
    "import multiprocessing as mp\n",
    "import dask.distributed as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import check_random_state\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from datacube.utils import geometry\n",
    "from sklearn.base import ClusterMixin\n",
    "from dask.diagnostics import ProgressBar\n",
    "from rasterio.features import rasterize\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "import warnings\n",
    "\n",
    "#from dea_tools.spatial import xr_rasterize\n",
    "\n",
    "\n",
    "def sklearn_flatten(input_xr):\n",
    "    \"\"\"\n",
    "    Reshape a DataArray or Dataset with spatial (and optionally\n",
    "    temporal) structure into an np.array with the spatial and temporal\n",
    "    dimensions flattened into one dimension.\n",
    "\n",
    "    This flattening procedure enables DataArrays and Datasets to be used\n",
    "    to train and predict\n",
    "    with sklearn models.\n",
    "\n",
    "    Last modified: September 2019\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_xr : xarray.DataArray or xarray.Dataset\n",
    "        Must have dimensions 'x' and 'y', may have dimension 'time'.\n",
    "        Dimensions other than 'x', 'y' and 'time' are unaffected by the\n",
    "        flattening.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    input_np : numpy.array\n",
    "        A numpy array corresponding to input_xr.data (or\n",
    "        input_xr.to_array().data), with dimensions 'x','y' and 'time'\n",
    "        flattened into a single dimension, which is the first axis of\n",
    "        the returned array. input_np contains no NaNs.\n",
    "\n",
    "    \"\"\"\n",
    "    # cast input Datasets to DataArray\n",
    "    if isinstance(input_xr, xr.Dataset):\n",
    "        input_xr = input_xr.to_array()\n",
    "\n",
    "    # stack across pixel dimensions, handling timeseries if necessary\n",
    "    if \"time\" in input_xr.dims:\n",
    "        stacked = input_xr.stack(z=[\"x\", \"y\", \"time\"])\n",
    "    else:\n",
    "        stacked = input_xr.stack(z=[\"x\", \"y\"])\n",
    "\n",
    "    # finding 'bands' dimensions in each pixel - these will not be\n",
    "    # flattened as their context is important for sklearn\n",
    "    pxdims = []\n",
    "    for dim in stacked.dims:\n",
    "        if dim != \"z\":\n",
    "            pxdims.append(dim)\n",
    "\n",
    "    # mask NaNs - we mask pixels with NaNs in *any* band, because\n",
    "    # sklearn cannot accept NaNs as input\n",
    "    mask = np.isnan(stacked)\n",
    "    if len(pxdims) != 0:\n",
    "        mask = mask.any(dim=pxdims)\n",
    "\n",
    "    # turn the mask into a numpy array (boolean indexing with xarrays\n",
    "    # acts weird)\n",
    "    mask = mask.data\n",
    "\n",
    "    # the dimension we are masking along ('z') needs to be the first\n",
    "    # dimension in the underlying np array for the boolean indexing to work\n",
    "    stacked = stacked.transpose(\"z\", *pxdims)\n",
    "    input_np = stacked.data[~mask]\n",
    "\n",
    "    return input_np\n",
    "\n",
    "\n",
    "def sklearn_unflatten(output_np, input_xr):\n",
    "    \"\"\"\n",
    "    Reshape a numpy array with no 'missing' elements (NaNs) and\n",
    "    'flattened' spatiotemporal structure into a DataArray matching the\n",
    "    spatiotemporal structure of the DataArray\n",
    "\n",
    "    This enables an sklearn model's prediction to be remapped to the\n",
    "    correct pixels in the input DataArray or Dataset.\n",
    "\n",
    "    Last modified: September 2019\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_np : numpy.array\n",
    "        The first dimension's length should correspond to the number of\n",
    "        valid (non-NaN) pixels in input_xr.\n",
    "    input_xr : xarray.DataArray or xarray.Dataset\n",
    "        Must have dimensions 'x' and 'y', may have dimension 'time'.\n",
    "        Dimensions other than 'x', 'y' and 'time' are unaffected by the\n",
    "        flattening.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    output_xr : xarray.DataArray\n",
    "        An xarray.DataArray with the same dimensions 'x', 'y' and 'time'\n",
    "        as input_xr, and the same valid (non-NaN) pixels. These pixels\n",
    "        are set to match the data in output_np.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # the output of a sklearn model prediction should just be a numpy array\n",
    "    # with size matching x*y*time for the input DataArray/Dataset.\n",
    "\n",
    "    # cast input Datasets to DataArray\n",
    "    if isinstance(input_xr, xr.Dataset):\n",
    "        input_xr = input_xr.to_array()\n",
    "\n",
    "    # generate the same mask we used to create the input to the sklearn model\n",
    "    if \"time\" in input_xr.dims:\n",
    "        stacked = input_xr.stack(z=[\"x\", \"y\", \"time\"])\n",
    "    else:\n",
    "        stacked = input_xr.stack(z=[\"x\", \"y\"])\n",
    "\n",
    "    pxdims = []\n",
    "    for dim in stacked.dims:\n",
    "        if dim != \"z\":\n",
    "            pxdims.append(dim)\n",
    "\n",
    "    mask = np.isnan(stacked)\n",
    "    if len(pxdims) != 0:\n",
    "        mask = mask.any(dim=pxdims)\n",
    "\n",
    "    # handle multivariable output\n",
    "    output_px_shape = ()\n",
    "    if len(output_np.shape[1:]):\n",
    "        output_px_shape = output_np.shape[1:]\n",
    "\n",
    "    # use the mask to put the data in all the right places\n",
    "    output_ma = np.ma.empty((len(stacked.z), *output_px_shape))\n",
    "    output_ma[~mask] = output_np\n",
    "    output_ma[mask] = np.ma.masked\n",
    "\n",
    "    # set the stacked coordinate to match the input\n",
    "    output_xr = xr.DataArray(\n",
    "        output_ma,\n",
    "        coords={\"z\": stacked[\"z\"]},\n",
    "        dims=[\"z\", *[\"output_dim_\" + str(idx) for idx in range(len(output_px_shape))]],\n",
    "    )\n",
    "\n",
    "    output_xr = output_xr.unstack()\n",
    "\n",
    "    return output_xr\n",
    "\n",
    "\n",
    "def fit_xr(model, input_xr):\n",
    "    \"\"\"\n",
    "    Utilise our wrappers to fit a vanilla sklearn model.\n",
    "\n",
    "    Last modified: September 2019\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn model or compatible object\n",
    "        Must have a fit() method that takes numpy arrays.\n",
    "    input_xr : xarray.DataArray or xarray.Dataset.\n",
    "        Must have dimensions 'x' and 'y', may have dimension 'time'.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    model : a scikit-learn model which has been fitted to the data in\n",
    "    the pixels of input_xr.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.fit(sklearn_flatten(input_xr))\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_xr(\n",
    "    model,\n",
    "    input_xr,\n",
    "    chunk_size=None,\n",
    "    persist=False,\n",
    "    proba=False,\n",
    "    clean=False,\n",
    "    return_input=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Using dask-ml ParallelPostfit(), runs  the parallel\n",
    "    predict and predict_proba methods of sklearn\n",
    "    estimators. Useful for running predictions\n",
    "    on a larger-than-RAM datasets.\n",
    "\n",
    "    Last modified: September 2020\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn model or compatible object\n",
    "        Must have a .predict() method that takes numpy arrays.\n",
    "    input_xr : xarray.DataArray or xarray.Dataset.\n",
    "        Must have dimensions 'x' and 'y'\n",
    "    chunk_size : int\n",
    "        The dask chunk size to use on the flattened array. If this\n",
    "        is left as None, then the chunks size is inferred from the\n",
    "        .chunks method on the `input_xr`\n",
    "    persist : bool\n",
    "        If True, and proba=True, then 'input_xr' data will be\n",
    "        loaded into distributed memory. This will ensure data\n",
    "        is not loaded twice for the prediction of probabilities,\n",
    "        but this will only work if the data is not larger than\n",
    "        distributed RAM.\n",
    "    proba : bool\n",
    "        If True, predict probabilities\n",
    "    clean : bool\n",
    "        If True, remove Infs and NaNs from input and output arrays\n",
    "    return_input : bool\n",
    "        If True, then the data variables in the 'input_xr' dataset will\n",
    "        be appended to the output xarray dataset.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    output_xr : xarray.Dataset\n",
    "        An xarray.Dataset containing the prediction output from model.\n",
    "        if proba=True then dataset will also contain probabilites, and\n",
    "        if return_input=True then dataset will have the input feature layers.\n",
    "        Has the same spatiotemporal structure as input_xr.\n",
    "\n",
    "    \"\"\"\n",
    "    # if input_xr isn't dask, coerce it\n",
    "    dask = True\n",
    "    if not bool(input_xr.chunks):\n",
    "        dask = False\n",
    "        input_xr = input_xr.chunk({\"x\": len(input_xr.x), \"y\": len(input_xr.y)})\n",
    "\n",
    "    # set chunk size if not supplied\n",
    "    if chunk_size is None:\n",
    "        chunk_size = int(input_xr.chunks[\"x\"][0]) * int(input_xr.chunks[\"y\"][0])\n",
    "\n",
    "    def _predict_func(model, input_xr, persist, proba, clean, return_input):\n",
    "        x, y, crs = input_xr.x, input_xr.y, input_xr.geobox.crs\n",
    "\n",
    "        input_data = []\n",
    "\n",
    "        for var_name in input_xr.data_vars:\n",
    "            input_data.append(input_xr[var_name])\n",
    "\n",
    "        input_data_flattened = []\n",
    "\n",
    "        for arr in input_data:\n",
    "            data = arr.data.flatten().rechunk(chunk_size)\n",
    "            input_data_flattened.append(data)\n",
    "\n",
    "        # reshape for prediction\n",
    "        input_data_flattened = da.array(input_data_flattened).transpose()\n",
    "\n",
    "        if clean == True:\n",
    "            input_data_flattened = da.where(\n",
    "                da.isfinite(input_data_flattened), input_data_flattened, 0\n",
    "            )\n",
    "\n",
    "        if (proba == True) & (persist == True):\n",
    "            # persisting data so we don't require loading all the data twice\n",
    "            input_data_flattened = input_data_flattened.persist()\n",
    "\n",
    "        # apply the classification\n",
    "        print(\"predicting...\")\n",
    "        out_class = model.predict(input_data_flattened)\n",
    "\n",
    "        # Mask out NaN or Inf values in results\n",
    "        if clean == True:\n",
    "            out_class = da.where(da.isfinite(out_class), out_class, 0)\n",
    "\n",
    "        # Reshape when writing out\n",
    "        out_class = out_class.reshape(len(y), len(x))\n",
    "\n",
    "        # stack back into xarray\n",
    "        output_xr = xr.DataArray(out_class, coords={\"x\": x, \"y\": y}, dims=[\"y\", \"x\"])\n",
    "\n",
    "        output_xr = output_xr.to_dataset(name=\"Predictions\")\n",
    "\n",
    "        if proba == True:\n",
    "            print(\"   probabilities...\")\n",
    "            out_proba = model.predict_proba(input_data_flattened)\n",
    "\n",
    "            # convert to %\n",
    "            out_proba = da.max(out_proba, axis=1) * 100.0\n",
    "\n",
    "            if clean == True:\n",
    "                out_proba = da.where(da.isfinite(out_proba), out_proba, 0)\n",
    "\n",
    "            out_proba = out_proba.reshape(len(y), len(x))\n",
    "\n",
    "            out_proba = xr.DataArray(\n",
    "                out_proba, coords={\"x\": x, \"y\": y}, dims=[\"y\", \"x\"]\n",
    "            )\n",
    "            output_xr[\"Probabilities\"] = out_proba\n",
    "\n",
    "        if return_input == True:\n",
    "            print(\"   input features...\")\n",
    "            # unflatten the input_data_flattened array and append\n",
    "            # to the output_xr containin the predictions\n",
    "            arr = input_xr.to_array()\n",
    "            stacked = arr.stack(z=[\"y\", \"x\"])\n",
    "\n",
    "            # handle multivariable output\n",
    "            output_px_shape = ()\n",
    "            if len(input_data_flattened.shape[1:]):\n",
    "                output_px_shape = input_data_flattened.shape[1:]\n",
    "\n",
    "            output_features = input_data_flattened.reshape(\n",
    "                (len(stacked.z), *output_px_shape)\n",
    "            )\n",
    "\n",
    "            # set the stacked coordinate to match the input\n",
    "            output_features = xr.DataArray(\n",
    "                output_features,\n",
    "                coords={\"z\": stacked[\"z\"]},\n",
    "                dims=[\n",
    "                    \"z\",\n",
    "                    *[\"output_dim_\" + str(idx) for idx in range(len(output_px_shape))],\n",
    "                ],\n",
    "            ).unstack()\n",
    "\n",
    "            # convert to dataset and rename arrays\n",
    "            output_features = output_features.to_dataset(dim=\"output_dim_0\")\n",
    "            data_vars = list(input_xr.data_vars)\n",
    "            output_features = output_features.rename(\n",
    "                {i: j for i, j in zip(output_features.data_vars, data_vars)}\n",
    "            )\n",
    "\n",
    "            # merge with predictions\n",
    "            output_xr = xr.merge([output_xr, output_features], compat=\"override\")\n",
    "\n",
    "        return assign_crs(output_xr, str(crs))\n",
    "\n",
    "    if dask == True:\n",
    "        # convert model to dask predict\n",
    "        model = ParallelPostFit(model)\n",
    "        with joblib.parallel_backend(\"dask\"):\n",
    "            output_xr = _predict_func(\n",
    "                model, input_xr, persist, proba, clean, return_input\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        output_xr = _predict_func(\n",
    "            model, input_xr, persist, proba, clean, return_input\n",
    "        ).compute()\n",
    "\n",
    "    return output_xr\n",
    "\n",
    "\n",
    "class HiddenPrints:\n",
    "    \"\"\"\n",
    "    For concealing unwanted print statements called by other functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "\n",
    "def _get_training_data_for_shp(\n",
    "    gdf,\n",
    "    index,\n",
    "    row,\n",
    "    out_arrs,\n",
    "    out_vars,\n",
    "    dc_query,\n",
    "    return_coords,\n",
    "    feature_func=None,\n",
    "    field=None,\n",
    "    zonal_stats=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This is the core function that is triggered by `collect_training_data`.\n",
    "    The `collect_training_data` function loops through geometries in a geopandas\n",
    "    geodataframe and runs the code within `_get_training_data_for_shp`.\n",
    "    Parameters are inherited from `collect_training_data`.\n",
    "    See that function for information on the other params not listed below.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index, row : iterables inherited from geopandas object\n",
    "    out_arrs : list\n",
    "        An empty list into which the training data arrays are stored.\n",
    "    out_vars : list\n",
    "        An empty list into which the data varaible names are stored.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    Two lists, a list of numpy.arrays containing classes and extracted data for\n",
    "    each pixel or polygon, and another containing the data variable names.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # prevent function altering dictionary kwargs\n",
    "    dc_query = deepcopy(dc_query)\n",
    "\n",
    "    # remove dask chunks if supplied as using\n",
    "    # mulitprocessing for parallization\n",
    "    if \"dask_chunks\" in dc_query.keys():\n",
    "        dc_query.pop(\"dask_chunks\", None)\n",
    "    \n",
    "    # set up query based on polygon\n",
    "    geom = geometry.Geometry(geom=gdf.iloc[index].geometry, crs=gdf.crs)\n",
    "    q = {\"geopolygon\": geom}\n",
    "\n",
    "    # merge polygon query with user supplied query params\n",
    "    dc_query.update(q)\n",
    "\n",
    "    # Use input feature function\n",
    "    data = feature_func(dc_query)\n",
    "\n",
    "    # create polygon mask\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], data)\n",
    "    data = data.where(mask)\n",
    "\n",
    "    # Check that feature_func has removed time\n",
    "    if \"time\" in data.dims:\n",
    "        t = data.dims[\"time\"]\n",
    "        if t > 1:\n",
    "            raise ValueError(\n",
    "                \"After running the feature_func, the dataset still has \"\n",
    "                + str(t)\n",
    "                + \" time-steps, dataset must only have\"\n",
    "                + \" x and y dimensions.\"\n",
    "            )\n",
    "\n",
    "    if return_coords == True:\n",
    "        # turn coords into a variable in the ds\n",
    "        data[\"x_coord\"] = data.x + 0 * data.y\n",
    "        data[\"y_coord\"] = data.y + 0 * data.x\n",
    "\n",
    "    # append ID measurement to dataset for tracking failures\n",
    "    band = [m for m in data.data_vars][0]\n",
    "    _id = xr.zeros_like(data[band])\n",
    "    data[\"id\"] = _id\n",
    "    data[\"id\"] = data[\"id\"] + gdf.iloc[index][\"id\"]\n",
    "\n",
    "    # If no zonal stats were requested then extract all pixel values\n",
    "    if zonal_stats is None:\n",
    "        flat_train = sklearn_flatten(data)\n",
    "        flat_val = np.repeat(row[field], flat_train.shape[0])\n",
    "        stacked = np.hstack((np.expand_dims(flat_val, axis=1), flat_train))\n",
    "\n",
    "    elif zonal_stats in [\"mean\", \"median\", \"max\", \"min\"]:\n",
    "        method_to_call = getattr(data, zonal_stats)\n",
    "        flat_train = method_to_call()\n",
    "        flat_train = flat_train.to_array()\n",
    "        stacked = np.hstack((row[field], flat_train))\n",
    "\n",
    "    else:\n",
    "        raise Exception(\n",
    "            zonal_stats\n",
    "            + \" is not one of the supported\"\n",
    "            + \" reduce functions ('mean','median','max','min')\"\n",
    "        )\n",
    "\n",
    "    out_arrs.append(stacked)\n",
    "    out_vars.append([field] + list(data.data_vars))\n",
    "\n",
    "\n",
    "def _get_training_data_parallel(\n",
    "    gdf, dc_query, ncpus, return_coords, feature_func=None, field=None, zonal_stats=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Function passing the '_get_training_data_for_shp' function\n",
    "    to a mulitprocessing.Pool.\n",
    "    Inherits variables from 'collect_training_data()'.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if dask-client is running\n",
    "    try:\n",
    "        zx = None\n",
    "        zx = dd.get_client()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if zx is not None:\n",
    "        raise ValueError(\n",
    "            \"You have a Dask Client running, which prevents \\n\"\n",
    "            \"this function from multiprocessing. Close the client.\"\n",
    "        )\n",
    "\n",
    "    # instantiate lists that can be shared across processes\n",
    "    manager = mp.Manager()\n",
    "    results = manager.list()\n",
    "    column_names = manager.list()\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(gdf))\n",
    "\n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    with mp.Pool(ncpus) as pool:\n",
    "        for index, row in gdf.iterrows():\n",
    "            pool.apply_async(\n",
    "                _get_training_data_for_shp,\n",
    "                [\n",
    "                    gdf,\n",
    "                    index,\n",
    "                    row,\n",
    "                    results,\n",
    "                    column_names,\n",
    "                    dc_query,\n",
    "                    return_coords,\n",
    "                    feature_func,\n",
    "                    field,\n",
    "                    zonal_stats,\n",
    "                ],\n",
    "                callback=update,\n",
    "            )\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pbar.close()\n",
    "\n",
    "    return column_names, results\n",
    "\n",
    "\n",
    "def collect_training_data(\n",
    "    gdf,\n",
    "    dc_query,\n",
    "    ncpus=1,\n",
    "    return_coords=False,\n",
    "    feature_func=None,\n",
    "    field=None,\n",
    "    zonal_stats=None,\n",
    "    clean=True,\n",
    "    fail_threshold=0.02,\n",
    "    fail_ratio=0.5,\n",
    "    max_retries=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function provides methods for gathering training data from the ODC over \n",
    "    geometries stored within a geopandas geodataframe. The function will return a\n",
    "    'model_input' array containing stacked training data arrays with all NaNs & Infs removed.\n",
    "    In the instance where ncpus > 1, a parallel version of the function will be run\n",
    "    (functions are passed to a mp.Pool()). This function can conduct zonal statistics if\n",
    "    the supplied shapefile contains polygons. The 'feature_func' parameter defines what\n",
    "    features to produce.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : geopandas geodataframe\n",
    "        geometry data in the form of a geopandas geodataframe\n",
    "    dc_query : dictionary\n",
    "        Datacube query object, should not contain lat and long (x or y)\n",
    "        variables as these are supplied by the 'gdf' variable\n",
    "    ncpus : int\n",
    "        The number of cpus/processes over which to parallelize the gathering\n",
    "        of training data (only if ncpus is > 1). Use 'mp.cpu_count()' to determine the number of\n",
    "        cpus available on a machine. Defaults to 1.\n",
    "    return_coords : bool\n",
    "        If True, then the training data will contain two extra columns 'x_coord' and\n",
    "        'y_coord' corresponding to the x,y coordinate of each sample. This variable can\n",
    "        be useful for handling spatial autocorrelation between samples later in the ML workflow.\n",
    "    feature_func : function\n",
    "        A function for generating feature layers that is applied to the data within\n",
    "        the bounds of the input geometry. The 'feature_func' must accept a 'dc_query'\n",
    "        object, and return a single xarray.Dataset or xarray.DataArray containing\n",
    "        2D coordinates (i.e x, y - no time dimension).\n",
    "        e.g.\n",
    "            def feature_function(query):\n",
    "                dc = datacube.Datacube(app='feature_layers')\n",
    "                ds = dc.load(**query)\n",
    "                ds = ds.mean('time')\n",
    "                return ds\n",
    "\n",
    "    field : str\n",
    "        Name of the column in the gdf that contains the class labels\n",
    "    zonal_stats : string, optional\n",
    "        An optional string giving the names of zonal statistics to calculate\n",
    "        for each polygon. Default is None (all pixel values are returned). Supported\n",
    "        values are 'mean', 'median', 'max', 'min'.\n",
    "    clean : bool\n",
    "        Whether or not to remove missing values in the training dataset. If True,\n",
    "        training labels with any NaNs or Infs in the feature layers will be dropped\n",
    "        from the dataset.\n",
    "    fail_threshold : float, default 0.02\n",
    "        Silent read fails on S3 can result in some rows of the returned data containing NaN values.\n",
    "        The'fail_threshold' fraction specifies a % of acceptable fails.\n",
    "        e.g. Setting 'fail_threshold' to 0.05 means if >5% of the samples in the training dataset\n",
    "        fail then those samples will be reutnred to the multiprocessing queue. Below this fraction\n",
    "        the function will accept the failures and return the results.\n",
    "    fail_ratio: float\n",
    "        A float between 0 and 1 that defines if a given training sample has failed.\n",
    "        Default is 0.5, which means if 50 % of the measurements in a given sample return null\n",
    "        values, and the number of total fails is more than the fail_threshold, the samplewill be\n",
    "        passed to the retry queue.\n",
    "    max_retries: int, default 3\n",
    "        Maximum number of times to retry collecting samples. This number is invoked\n",
    "        if the 'fail_threshold' is not reached.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    Two lists, a list of numpy.arrays containing classes and extracted data for\n",
    "    each pixel or polygon, and another containing the data variable names.\n",
    "    \"\"\"\n",
    "\n",
    "    # check the dtype of the class field\n",
    "    if gdf[field].dtype != np.int:\n",
    "        raise ValueError(\n",
    "            'The \"field\" column of the input vector must contain integer dtypes'\n",
    "        )\n",
    "\n",
    "    # check for feature_func\n",
    "    if feature_func is None:\n",
    "         raise ValueError(\n",
    "            \"Please supply a feature layer function through the \"\n",
    "            +\"parameter 'feature_func'\"\n",
    "        )\n",
    "\n",
    "    if zonal_stats is not None:\n",
    "        print(\"Taking zonal statistic: \" + zonal_stats)\n",
    "    \n",
    "    # add unique id to gdf to help with indexing failed rows\n",
    "    # during multiprocessing\n",
    "    # if zonal_stats is not None:\n",
    "    gdf[\"id\"] = range(0, len(gdf))\n",
    "\n",
    "    if ncpus == 1:\n",
    "        # progress indicator\n",
    "        print(\"Collecting training data in serial mode\")\n",
    "        i = 0\n",
    "\n",
    "        # list to store results\n",
    "        results = []\n",
    "        column_names = []\n",
    "\n",
    "        # loop through polys and extract training data\n",
    "        for index, row in gdf.iterrows():\n",
    "            print(\" Feature {:04}/{:04}\\r\".format(i + 1, len(gdf)), end=\"\")\n",
    "\n",
    "            _get_training_data_for_shp(\n",
    "                gdf,\n",
    "                index,\n",
    "                row,\n",
    "                results,\n",
    "                column_names,\n",
    "                dc_query,\n",
    "                return_coords,\n",
    "                feature_func,\n",
    "                field,\n",
    "                zonal_stats,\n",
    "            )\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        print(\"Collecting training data in parallel mode\")\n",
    "        column_names, results = _get_training_data_parallel(\n",
    "            gdf=gdf,\n",
    "            dc_query=dc_query,\n",
    "            ncpus=ncpus,\n",
    "            return_coords=return_coords,\n",
    "            feature_func=feature_func,\n",
    "            field=field,\n",
    "            zonal_stats=zonal_stats,\n",
    "        )\n",
    "\n",
    "    # column names are appended during each iteration\n",
    "    # but they are identical, grab only the first instance\n",
    "    column_names = column_names[0]\n",
    "\n",
    "    # Stack the extracted training data for each feature into a single array\n",
    "    model_input = np.vstack(results)\n",
    "\n",
    "    # this code block below iteratively retries failed rows\n",
    "    # up to max_retries or until fail_threshold is\n",
    "    # reached - whichever occurs first\n",
    "    if ncpus > 1:\n",
    "        i = 1\n",
    "        while i <= max_retries:\n",
    "            # Find % of fails (null values) in data. Use Pandas for simplicity\n",
    "            df = pd.DataFrame(data=model_input[:, 0:-1], index=model_input[:, -1])\n",
    "            # how many nan values per id?\n",
    "            num_nans = df.isnull().sum(axis=1)\n",
    "            num_nans = num_nans.groupby(num_nans.index).sum()\n",
    "            # how many valid values per id?\n",
    "            num_valid = df.notnull().sum(axis=1)\n",
    "            num_valid = num_valid.groupby(num_valid.index).sum()\n",
    "            # find fail rate\n",
    "            perc_fail = num_nans / (num_nans + num_valid)\n",
    "            fail_ids = perc_fail[perc_fail > fail_ratio]\n",
    "            fail_rate = len(fail_ids) / len(gdf)\n",
    "\n",
    "            print(\n",
    "                \"Percentage of possible fails after run \"\n",
    "                + str(i)\n",
    "                + \" = \"\n",
    "                + str(round(fail_rate * 100, 2))\n",
    "                + \" %\"\n",
    "            )\n",
    "\n",
    "            if fail_rate > fail_threshold:\n",
    "                print(\"Recollecting samples that failed\")\n",
    "\n",
    "                fail_ids = list(fail_ids.index)\n",
    "                # keep only the ids in model_input object that didn't fail\n",
    "                model_input = model_input[~np.isin(model_input[:, -1], fail_ids)]\n",
    "\n",
    "                # index out the fail_ids from the original gdf\n",
    "                gdf_rerun = gdf.loc[gdf[\"id\"].isin(fail_ids)]\n",
    "                gdf_rerun = gdf_rerun.reset_index(drop=True)\n",
    "\n",
    "                time.sleep(5)  # sleep for 5s to rest api\n",
    "\n",
    "                # recollect failed rows\n",
    "                column_names_again, results_again = _get_training_data_parallel(\n",
    "                    gdf=gdf_rerun,\n",
    "                    dc_query=dc_query,\n",
    "                    ncpus=ncpus,\n",
    "                    return_coords=return_coords,\n",
    "                    feature_func=feature_func,\n",
    "                    field=field,\n",
    "                    zonal_stats=zonal_stats,\n",
    "                )\n",
    "\n",
    "                # Stack the extracted training data for each feature into a single array\n",
    "                model_input_again = np.vstack(results_again)\n",
    "\n",
    "                # merge results of the re-run with original run\n",
    "                model_input = np.vstack((model_input, model_input_again))\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # remove id column\n",
    "    idx_var = column_names[0:-1]\n",
    "    model_col_indices = [column_names.index(var_name) for var_name in idx_var]\n",
    "    model_input = model_input[:, model_col_indices]\n",
    "\n",
    "    if clean == True:\n",
    "        num = np.count_nonzero(np.isnan(model_input).any(axis=1))\n",
    "        model_input = model_input[~np.isnan(model_input).any(axis=1)]\n",
    "        model_input = model_input[~np.isinf(model_input).any(axis=1)]\n",
    "        print(\"Removed \" + str(num) + \" rows wth NaNs &/or Infs\")\n",
    "        print(\"Output shape: \", model_input.shape)\n",
    "\n",
    "    else:\n",
    "        print(\"Returning data without cleaning\")\n",
    "        print(\"Output shape: \", model_input.shape)\n",
    "\n",
    "    return column_names[0:-1], model_input\n",
    "\n",
    "\n",
    "class KMeans_tree(ClusterMixin):\n",
    "    \"\"\"\n",
    "    A hierarchical KMeans unsupervised clustering model. This class is\n",
    "    a clustering model, so it inherits scikit-learn's ClusterMixin\n",
    "    base class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_levels : integer, default 2\n",
    "        number of levels in the tree of clustering models.\n",
    "    n_clusters : integer, default 3\n",
    "        Number of clusters in each of the constituent KMeans models in\n",
    "        the tree.\n",
    "    **kwargs : optional\n",
    "        Other keyword arguments to be passed directly to the KMeans\n",
    "        initialiser.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_levels=2, n_clusters=3, **kwargs):\n",
    "\n",
    "        assert n_levels >= 1\n",
    "\n",
    "        self.base_model = KMeans(n_clusters=3, **kwargs)\n",
    "        self.n_levels = n_levels\n",
    "        self.n_clusters = n_clusters\n",
    "        # make child models\n",
    "        if n_levels > 1:\n",
    "            self.branches = [\n",
    "                KMeans_tree(n_levels=n_levels - 1, n_clusters=n_clusters, **kwargs)\n",
    "                for _ in range(n_clusters)\n",
    "            ]\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Fit the tree of KMeans models. All parameters mimic those\n",
    "        of KMeans.fit().\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
    "            Training instances to cluster. It must be noted that the\n",
    "            data will be converted to C ordering, which will cause a\n",
    "            memory copy if the given data is not C-contiguous.\n",
    "        y : Ignored\n",
    "            not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all\n",
    "            observations are assigned equal weight (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels_ = self.base_model.fit(X, sample_weight=sample_weight).labels_\n",
    "\n",
    "        if self.n_levels > 1:\n",
    "            labels_old = np.copy(self.labels_)\n",
    "            # make room to add the sub-cluster labels\n",
    "            self.labels_ *= (self.n_clusters) ** (self.n_levels - 1)\n",
    "\n",
    "            for clu in range(self.n_clusters):\n",
    "                # fit child models on their corresponding partition of the training set\n",
    "                self.branches[clu].fit(\n",
    "                    X[labels_old == clu],\n",
    "                    sample_weight=(\n",
    "                        sample_weight[labels_old == clu]\n",
    "                        if sample_weight is not None\n",
    "                        else None\n",
    "                    ),\n",
    "                )\n",
    "                self.labels_[labels_old == clu] += self.branches[clu].labels_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Send X through the KMeans tree and predict the resultant\n",
    "        cluster. Compatible with KMeans.predict().\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            New data to predict.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all\n",
    "            observations are assigned equal weight (default: None)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape [n_samples,]\n",
    "            Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.base_model.predict(X, sample_weight=sample_weight)\n",
    "\n",
    "        if self.n_levels > 1:\n",
    "            rescpy = np.copy(result)\n",
    "\n",
    "            # make room to add the sub-cluster labels\n",
    "            result *= (self.n_clusters) ** (self.n_levels - 1)\n",
    "\n",
    "            for clu in range(self.n_clusters):\n",
    "                result[rescpy == clu] += self.branches[clu].predict(\n",
    "                    X[rescpy == clu],\n",
    "                    sample_weight=(\n",
    "                        sample_weight[rescpy == clu]\n",
    "                        if sample_weight is not None\n",
    "                        else None\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def spatial_clusters(\n",
    "    coordinates,\n",
    "    method=\"Hierarchical\",\n",
    "    max_distance=None,\n",
    "    n_groups=None,\n",
    "    verbose=False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Create spatial groups on coorindate data using either KMeans clustering\n",
    "    or a Gaussian Mixture model\n",
    "    Last modified: September 2020\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_groups : int\n",
    "        The number of groups to create. This is passed as 'n_clusters=n_groups'\n",
    "        for the KMeans algo, and 'n_components=n_groups' for the GMM. If using\n",
    "        method='Hierarchical' then this paramter is ignored.\n",
    "    coordinates : np.array\n",
    "        A numpy array of coordinate values e.g.\n",
    "        np.array([[3337270.,  262400.],\n",
    "                  [3441390., -273060.], ...])\n",
    "    method : str\n",
    "        Which algorithm to use to seperate data points. Either 'KMeans', 'GMM', or\n",
    "        'Hierarchical'. If using 'Hierarchical' then must set max_distance.\n",
    "    max_distance : int\n",
    "        If method is set to 'hierarchical' then maximum distance describes the\n",
    "        maximum euclidean distances between all observations in a cluster. 'n_groups'\n",
    "        is ignored in this case.\n",
    "    **kwargs : optional,\n",
    "        Additional keyword arguments to pass to sklearn.cluster.Kmeans or\n",
    "        sklearn.mixture.GuassianMixture depending on the 'method' argument.\n",
    "    Returns\n",
    "    -------\n",
    "     labels : array, shape [n_samples,]\n",
    "        Index of the cluster each sample belongs to.\n",
    "    \"\"\"\n",
    "    if method not in [\"Hierarchical\", \"KMeans\", \"GMM\"]:\n",
    "        raise ValueError(\"method must be one of: 'Hierarchical','KMeans' or 'GMM'\")\n",
    "\n",
    "    if (method in [\"GMM\", \"KMeans\"]) & (n_groups is None):\n",
    "        raise ValueError(\n",
    "            \"The 'GMM' and 'KMeans' methods requires explicitly setting 'n_groups'\"\n",
    "        )\n",
    "\n",
    "    if (method == \"Hierarchical\") & (max_distance is None):\n",
    "        raise ValueError(\"The 'Hierarchical' method requires setting max_distance\")\n",
    "\n",
    "    if method == \"Hierarchical\":\n",
    "        cluster_label = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            linkage=\"complete\",\n",
    "            distance_threshold=max_distance,\n",
    "            **kwargs\n",
    "        ).fit_predict(coordinates)\n",
    "\n",
    "    if method == \"KMeans\":\n",
    "        cluster_label = KMeans(n_clusters=n_groups, **kwargs).fit_predict(coordinates)\n",
    "\n",
    "    if method == \"GMM\":\n",
    "        cluster_label = GaussianMixture(n_components=n_groups, **kwargs).fit_predict(\n",
    "            coordinates\n",
    "        )\n",
    "    if verbose:\n",
    "        print(\"n clusters = \" + str(len(np.unique(cluster_label))))\n",
    "\n",
    "    return cluster_label\n",
    "\n",
    "\n",
    "def SKCV(\n",
    "    coordinates,\n",
    "    n_splits,\n",
    "    cluster_method,\n",
    "    kfold_method,\n",
    "    test_size,\n",
    "    balance,\n",
    "    n_groups=None,\n",
    "    max_distance=None,\n",
    "    train_size=None,\n",
    "    random_state=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate spatial k-fold cross validation indices using coordinate data.\n",
    "    This function wraps the 'SpatialShuffleSplit' and 'SpatialKFold' classes.\n",
    "    These classes ingest coordinate data in the form of an\n",
    "    np.array([[Eastings, northings]]) and assign samples to a spatial cluster\n",
    "    using either a KMeans, Gaussain Mixture, or Agglomerative Clustering algorithm.\n",
    "    This cross-validator is preferred over other sklearn.model_selection methods\n",
    "    for spatial data to avoid overestimating cross-validation scores.\n",
    "    This can happen because of the inherent spatial autocorrelation that is usually\n",
    "    associated with this type of data.\n",
    "\n",
    "    Last modified: Dec 2020\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coordinates : np.array\n",
    "        A numpy array of coordinate values e.g.\n",
    "        np.array([[3337270.,  262400.],\n",
    "                  [3441390., -273060.], ...])\n",
    "    n_splits : int\n",
    "        The number of test-train cross validation splits to generate.\n",
    "    cluster_method : str\n",
    "        Which algorithm to use to seperate data points. Either 'KMeans', 'GMM', or\n",
    "        'Hierarchical'\n",
    "    kfold_method : str\n",
    "        One of either 'SpatialShuffleSplit' or 'SpatialKFold'. See the docs\n",
    "        under class:_SpatialShuffleSplit and class: _SpatialKFold for more\n",
    "        information on these options.\n",
    "    test_size : float, int, None\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. If ``train_size`` is also None, it will\n",
    "        be set to 0.15.\n",
    "    balance : int or bool\n",
    "        if setting kfold_method to 'SpatialShuffleSplit': int\n",
    "            The number of splits generated per iteration to try to balance the\n",
    "            amount of data in each set so that *test_size* and *train_size* are\n",
    "            respected. If 1, then no extra splits are generated (essentially\n",
    "            disabling the balacing). Must be >= 1.\n",
    "         if setting kfold_method to 'SpatialKFold': bool\n",
    "             Whether or not to split clusters into fold with approximately equal\n",
    "            number of data points. If False, each fold will have the same number of\n",
    "            clusters (which can have different number of data points in them).\n",
    "    n_groups : int\n",
    "        The number of groups to create. This is passed as 'n_clusters=n_groups'\n",
    "        for the KMeans algo, and 'n_components=n_groups' for the GMM. If using\n",
    "        cluster_method='Hierarchical' then this parameter is ignored.\n",
    "    max_distance : int\n",
    "        If method is set to 'hierarchical' then maximum distance describes the\n",
    "        maximum euclidean distances between all observations in a cluster. 'n_groups'\n",
    "        is ignored in this case.\n",
    "    train_size : float, int, or None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    **kwargs : optional,\n",
    "        Additional keyword arguments to pass to sklearn.cluster.Kmeans or\n",
    "        sklearn.mixture.GuassianMixture depending on the cluster_method argument.\n",
    "    Returns\n",
    "    --------\n",
    "    generator object _BaseSpatialCrossValidator.split\n",
    "\n",
    "    \"\"\"\n",
    "    # intiate a method\n",
    "    if kfold_method == \"SpatialShuffleSplit\":\n",
    "        splitter = _SpatialShuffleSplit(\n",
    "            n_groups=n_groups,\n",
    "            method=cluster_method,\n",
    "            coordinates=coordinates,\n",
    "            max_distance=max_distance,\n",
    "            test_size=test_size,\n",
    "            train_size=train_size,\n",
    "            n_splits=n_splits,\n",
    "            random_state=random_state,\n",
    "            balance=balance,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    if kfold_method == \"SpatialKFold\":\n",
    "        splitter = _SpatialKFold(\n",
    "            n_groups=n_groups,\n",
    "            coordinates=coordinates,\n",
    "            max_distance=max_distance,\n",
    "            method=cluster_method,\n",
    "            test_size=test_size,\n",
    "            n_splits=n_splits,\n",
    "            random_state=random_state,\n",
    "            balance=balance,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    return splitter\n",
    "\n",
    "\n",
    "def spatial_train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    coordinates,\n",
    "    cluster_method,\n",
    "    kfold_method,\n",
    "    balance,\n",
    "    test_size=None,\n",
    "    n_splits=None,\n",
    "    n_groups=None,\n",
    "    max_distance=None,\n",
    "    train_size=None,\n",
    "    random_state=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Split arrays into random train and test subsets. Similar to\n",
    "    `sklearn.model_selection.train_test_split` but instead works on\n",
    "    spatial coordinate data. Coordinate data is grouped according\n",
    "    to either a KMeans, Gaussain Mixture, or Agglomerative Clustering algorthim.\n",
    "    Grouping by spatial clusters is preferred over plain random splits for\n",
    "    spatial data to avoid overestimating validation scores due to spatial\n",
    "    autocorrelation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        Training data features\n",
    "    y : np.array\n",
    "        Training data labels\n",
    "    coordinates : np.array\n",
    "        A numpy array of coordinate values e.g.\n",
    "        np.array([[3337270.,  262400.],\n",
    "                  [3441390., -273060.], ...])\n",
    "    cluster_method : str\n",
    "        Which algorithm to use to seperate data points. Either 'KMeans', 'GMM', or\n",
    "        'Hierarchical'\n",
    "    kfold_method : str\n",
    "        One of either 'SpatialShuffleSplit' or 'SpatialKFold'. See the docs\n",
    "        under class:_SpatialShuffleSplit and class: _SpatialKFold for more\n",
    "        information on these options.\n",
    "    balance : int or bool\n",
    "        if setting kfold_method to 'SpatialShuffleSplit': int\n",
    "            The number of splits generated per iteration to try to balance the\n",
    "            amount of data in each set so that *test_size* and *train_size* are\n",
    "            respected. If 1, then no extra splits are generated (essentially\n",
    "            disabling the balacing). Must be >= 1.\n",
    "         if setting kfold_method to 'SpatialKFold': bool\n",
    "            Whether or not to split clusters into fold with approximately equal\n",
    "            number of data points. If False, each fold will have the same number of\n",
    "            clusters (which can have different number of data points in them).\n",
    "    test_size : float, int, None\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. If ``train_size`` is also None, it will\n",
    "        be set to 0.15.\n",
    "    n_splits : int\n",
    "        This parameter is invoked for the 'SpatialKFold' folding method, use this\n",
    "        number to satisfy the train-test size ratio desired, as the 'test_size'\n",
    "        parameter for the KFold method often fails to get the ratio right.\n",
    "    n_groups : int\n",
    "        The number of groups to create. This is passed as 'n_clusters=n_groups'\n",
    "        for the KMeans algo, and 'n_components=n_groups' for the GMM. If using\n",
    "        cluster_method='Hierarchical' then this parameter is ignored.\n",
    "    max_distance : int\n",
    "        If method is set to 'hierarchical' then maximum distance describes the\n",
    "        maximum euclidean distances between all observations in a cluster. 'n_groups'\n",
    "        is ignored in this case.\n",
    "    train_size : float, int, or None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int,\n",
    "        RandomState instance or None, optional\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    **kwargs : optional,\n",
    "        Additional keyword arguments to pass to sklearn.cluster.Kmeans or\n",
    "        sklearn.mixture.GuassianMixture depending on the cluster_method argument.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple :\n",
    "        Contains four arrays in the following order:\n",
    "            X_train, X_test, y_train, y_test\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if kfold_method == \"SpatialShuffleSplit\":\n",
    "        splitter = _SpatialShuffleSplit(\n",
    "            n_groups=n_groups,\n",
    "            method=cluster_method,\n",
    "            coordinates=coordinates,\n",
    "            max_distance=max_distance,\n",
    "            test_size=test_size,\n",
    "            train_size=train_size,\n",
    "            n_splits=1 if n_splits is None else n_splits,\n",
    "            random_state=random_state,\n",
    "            balance=balance,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    if kfold_method == \"SpatialKFold\":\n",
    "        if n_splits is None:\n",
    "            raise ValueError(\n",
    "                \"n_splits parameter requires an integer value, eg. 'n_splits=5'\"\n",
    "            )\n",
    "        if (test_size is not None) or (train_size is not None):\n",
    "            warnings.warn(\n",
    "                \"With the 'SpatialKFold' method, controlling the test/train ratio \"\n",
    "                \"is better achieved using the 'n_splits' parameter\"\n",
    "            )\n",
    "\n",
    "        splitter = _SpatialKFold(\n",
    "            n_groups=n_groups,\n",
    "            coordinates=coordinates,\n",
    "            max_distance=max_distance,\n",
    "            method=cluster_method,\n",
    "            n_splits=n_splits,\n",
    "            random_state=random_state,\n",
    "            balance=balance,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    lst = []\n",
    "    for train, test in splitter.split(coordinates):\n",
    "        X_tr, X_tt = X[train, :], X[test, :]\n",
    "        y_tr, y_tt = y[train], y[test]\n",
    "        lst.extend([X_tr, X_tt, y_tr, y_tt])\n",
    "\n",
    "    return (lst[0], lst[1], lst[2], lst[3])\n",
    "\n",
    "\n",
    "def _partition_by_sum(array, parts):\n",
    "    \"\"\"\n",
    "    Partition an array into parts of approximately equal sum.\n",
    "    Does not change the order of the array elements.\n",
    "    Produces the partition indices on the array. Use :func:`numpy.split` to\n",
    "    divide the array along these indices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array or array-like\n",
    "        The 1D array that will be partitioned. The array will be raveled before\n",
    "        computations.\n",
    "    parts : int\n",
    "        Number of parts to split the array. Can be at most the number of\n",
    "        elements in the array.\n",
    "    Returns\n",
    "    -------\n",
    "    indices : array\n",
    "        The indices in which the array should be split.\n",
    "    Notes\n",
    "    -----\n",
    "    Solution from https://stackoverflow.com/a/54024280\n",
    "    \"\"\"\n",
    "    array = np.atleast_1d(array).ravel()\n",
    "    if parts > array.size:\n",
    "        raise ValueError(\n",
    "            \"Cannot partition an array of size {} into {} parts of equal sum.\".format(\n",
    "                array.size, parts\n",
    "            )\n",
    "        )\n",
    "    cumulative_sum = array.cumsum()\n",
    "    # Ideally, we want each part to have the same number of points (total /\n",
    "    # parts).\n",
    "    ideal_sum = cumulative_sum[-1] // parts\n",
    "    # If the parts are ideal, the cumulative sum of each part will be this\n",
    "    ideal_cumsum = np.arange(1, parts) * ideal_sum\n",
    "    indices = np.searchsorted(cumulative_sum, ideal_cumsum, side=\"right\")\n",
    "    # Check for repeated split points, which indicates that there is no way to\n",
    "    # split the array.\n",
    "    if np.unique(indices).size != indices.size:\n",
    "        raise ValueError(\n",
    "            \"Could not find partition points to split the array into {} parts \"\n",
    "            \"of equal sum.\".format(parts)\n",
    "        )\n",
    "    return indices\n",
    "\n",
    "\n",
    "class _BaseSpatialCrossValidator(BaseCrossValidator, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Base class for spatial cross-validators.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_groups : int\n",
    "        The number of groups to create. This is passed as 'n_clusters=n_groups'\n",
    "        for the KMeans algo, and 'n_components=n_groups' for the GMM.\n",
    "    coordinates : np.array\n",
    "        A numpy array of coordinate values e.g.\n",
    "        np.array([[3337270.,  262400.],\n",
    "                  [3441390., -273060.], ...,\n",
    "    method : str\n",
    "        Which algorithm to use to seperate data points. Either 'KMeans' or 'GMM'\n",
    "    n_splits : int\n",
    "        Number of splitting iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_groups=None,\n",
    "        coordinates=None,\n",
    "        method=None,\n",
    "        max_distance=None,\n",
    "        n_splits=None,\n",
    "    ):\n",
    "\n",
    "        self.n_groups = n_groups\n",
    "        self.coordinates = coordinates\n",
    "        self.method = method\n",
    "        self.max_distance = max_distance\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Columns should be the easting and northing coordinates of data\n",
    "            points, respectively.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target variable for supervised learning problems. Always\n",
    "            ignored.\n",
    "        groups : array-like, with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set. Always ignored.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if X.shape[1] != 2:\n",
    "            raise ValueError(\n",
    "                \"X (the coordinate data) must have exactly 2 columns ({} given).\".format(\n",
    "                    X.shape[1]\n",
    "                )\n",
    "            )\n",
    "        for train, test in super().split(X, y, groups):\n",
    "            yield train, test\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Returns the number of splitting iterations in the cross-validator\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        y : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits\n",
    "\n",
    "    @abstractmethod\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Generates integer indices corresponding to test sets.\n",
    "        MUST BE IMPLEMENTED BY DERIVED CLASSES.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Columns should be the easting and northing coordinates of data\n",
    "            points, respectively.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target variable for supervised learning problems. Always\n",
    "            ignored.\n",
    "        groups : array-like, with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set. Always ignored.\n",
    "        Yields\n",
    "        ------\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class _SpatialShuffleSplit(_BaseSpatialCrossValidator):\n",
    "    \"\"\"\n",
    "    Random permutation of spatial cross-validator.\n",
    "    Yields indices to split data into training and test sets. Data are first\n",
    "    grouped into clusters using either a KMeans or GMM algorithm\n",
    "    and are then split into testing and training sets randomly.\n",
    "    The proportion of clusters assigned to each set is controlled by *test_size*\n",
    "    and/or *train_size*. However, the total amount of actual data points in\n",
    "    each set could be different from these values since clusters can have\n",
    "    a different number of data points inside them. To guarantee that the\n",
    "    proportion of actual data is as close as possible to the proportion of\n",
    "    clusters, this cross-validator generates an extra number of splits and\n",
    "    selects the one with proportion of data points in each set closer to the\n",
    "    desired amount. The number of balance splits per\n",
    "    iteration is controlled by the *balance* argument.\n",
    "    This cross-validator is preferred over `sklearn.model_selection.ShuffleSplit`\n",
    "    for spatial data to avoid overestimating cross-validation scores.\n",
    "    This can happen because of the inherent spatial autocorrelation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_groups : int\n",
    "        The number of groups to create. This is passed as 'n_clusters=n_groups'\n",
    "        for the KMeans algo, and 'n_components=n_groups' for the GMM. If using\n",
    "        cluster_method='Hierarchical' then this parameter is ignored.\n",
    "    coordinates : np.array\n",
    "        A numpy array of coordinate values e.g.\n",
    "        np.array([[3337270.,  262400.],\n",
    "                  [3441390., -273060.], ...])\n",
    "    cluster_method : str\n",
    "        Which algorithm to use to seperate data points. Either 'KMeans', 'GMM', or\n",
    "        'Hierarchical'\n",
    "    max_distance : int\n",
    "        If method is set to 'hierarchical' then maximum distance describes the\n",
    "        maximum euclidean distances between all observations in a cluster. 'n_groups'\n",
    "        is ignored in this case.\n",
    "    n_splits : int,\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    test_size : float, int, None\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. If ``train_size`` is also None, it will\n",
    "        be set to 0.1.\n",
    "    train_size : float, int, or None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    balance : int\n",
    "        The number of splits generated per iteration to try to balance the\n",
    "        amount of data in each set so that *test_size* and *train_size* are\n",
    "        respected. If 1, then no extra splits are generated (essentially\n",
    "        disabling the balacing). Must be >= 1.\n",
    "    **kwargs : optional,\n",
    "        Additional keyword arguments to pass to sklearn.cluster.Kmeans or\n",
    "        sklearn.mixture.GuassianMixture depending on the cluster_method argument.\n",
    "    Returns\n",
    "    --------\n",
    "    generator\n",
    "        containing indices to split data into training and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_groups=None,\n",
    "        coordinates=None,\n",
    "        method=\"Heirachical\",\n",
    "        max_distance=None,\n",
    "        n_splits=None,\n",
    "        test_size=0.15,\n",
    "        train_size=None,\n",
    "        random_state=None,\n",
    "        balance=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_groups=n_groups,\n",
    "            coordinates=coordinates,\n",
    "            method=method,\n",
    "            max_distance=max_distance,\n",
    "            n_splits=n_splits,\n",
    "            **kwargs\n",
    "        )\n",
    "        if balance < 1:\n",
    "            raise ValueError(\n",
    "                \"The *balance* argument must be >= 1. To disable balance, use 1.\"\n",
    "            )\n",
    "        self.test_size = test_size\n",
    "        self.train_size = train_size\n",
    "        self.random_state = random_state\n",
    "        self.balance = balance\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Generates integer indices corresponding to test sets.\n",
    "        Runs several iterations until a split is found that yields clusters with\n",
    "        the right amount of data points in it.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Columns should be the easting and northing coordinates of data\n",
    "            points, respectively.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target variable for supervised learning problems. Always\n",
    "            ignored.\n",
    "        groups : array-like, with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set. Always ignored.\n",
    "        Yields\n",
    "        ------\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        labels = spatial_clusters(\n",
    "            n_groups=self.n_groups,\n",
    "            coordinates=self.coordinates,\n",
    "            method=self.method,\n",
    "            max_distance=self.max_distance,\n",
    "            **self.kwargs\n",
    "        )\n",
    "\n",
    "        cluster_ids = np.unique(labels)\n",
    "        # Generate many more splits so that we can pick and choose the ones\n",
    "        # that have the right balance of training and testing data.\n",
    "        shuffle = ShuffleSplit(\n",
    "            n_splits=self.n_splits * self.balance,\n",
    "            test_size=self.test_size,\n",
    "            train_size=self.train_size,\n",
    "            random_state=self.random_state,\n",
    "        ).split(cluster_ids)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            test_sets, balance = [], []\n",
    "            for _ in range(self.balance):\n",
    "                # This is a false positive in pylint which is why the warning\n",
    "                # is disabled at the top of this file:\n",
    "                # https://github.com/PyCQA/pylint/issues/1830\n",
    "                # pylint: disable=stop-iteration-return\n",
    "                train_clusters, test_clusters = next(shuffle)\n",
    "                # pylint: enable=stop-iteration-return\n",
    "                train_points = np.where(np.isin(labels, cluster_ids[train_clusters]))[0]\n",
    "                test_points = np.where(np.isin(labels, cluster_ids[test_clusters]))[0]\n",
    "                # The proportion of data points assigned to each group should\n",
    "                # be close the proportion of clusters assigned to each group.\n",
    "                balance.append(\n",
    "                    abs(\n",
    "                        train_points.size / test_points.size\n",
    "                        - train_clusters.size / test_clusters.size\n",
    "                    )\n",
    "                )\n",
    "                test_sets.append(test_points)\n",
    "            best = np.argmin(balance)\n",
    "            yield test_sets[best]\n",
    "\n",
    "\n",
    "class _SpatialKFold(_BaseSpatialCrossValidator):\n",
    "    \"\"\"\n",
    "    Spatial K-Folds cross-validator.\n",
    "    Yields indices to split data into training and test sets. Data are first\n",
    "    grouped into clusters using either a KMeans or GMM algorithm\n",
    "    clusters. The clusters are then split into testing and training sets iteratively\n",
    "    along k folds of the data (k is given by *n_splits*).\n",
    "    By default, the clusters are split into folds in a way that makes each fold\n",
    "    have approximately the same number of data points. Sometimes this might not\n",
    "    be possible, which can happen if the number of splits is close to the\n",
    "    number of clusters. In these cases, each fold will have the same number of\n",
    "    clusters regardless of how many data points are in each cluster. This\n",
    "    behaviour can also be disabled by setting ``balance=False``.\n",
    "    This cross-validator is preferred over `sklearn.model_selection.KFold` for\n",
    "    spatial data to avoid overestimating cross-validation scores. This can happen\n",
    "    because of the inherent autocorrelation that is usually associated with\n",
    "    this type of data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_groups : int\n",
    "        The number of groups to create. This is passed as 'n_clusters=n_groups'\n",
    "        for the KMeans algo, and 'n_components=n_groups' for the GMM. If using\n",
    "        cluster_method='Hierarchical' then this parameter is ignored.\n",
    "    coordinates : np.array\n",
    "        A numpy array of coordinate values e.g.\n",
    "        np.array([[3337270.,  262400.],\n",
    "                  [3441390., -273060.], ...])\n",
    "    cluster_method : str\n",
    "        Which algorithm to use to seperate data points. Either 'KMeans', 'GMM', or\n",
    "        'Hierarchical'\n",
    "    max_distance : int\n",
    "        If method is set to 'hierarchical' then maximum distance describes the\n",
    "        maximum euclidean distances between all observations in a cluster. 'n_groups'\n",
    "        is ignored in this case.\n",
    "    n_splits : int\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the data before splitting into batches.\n",
    "    random_state : int, RandomState instance or None, optional (defasult=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    balance : bool\n",
    "        Whether or not to split clusters into fold with approximately equal\n",
    "        number of data points. If False, each fold will have the same number of\n",
    "        clusters (which can have different number of data points in them).\n",
    "    **kwargs : optional,\n",
    "        Additional keyword arguments to pass to sklearn.cluster.Kmeans or\n",
    "        sklearn.mixture.GuassianMixture depending on the cluster_method argument.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_groups=None,\n",
    "        coordinates=None,\n",
    "        method=\"Heirachical\",\n",
    "        max_distance=None,\n",
    "        n_splits=5,\n",
    "        test_size=0.15,\n",
    "        train_size=None,\n",
    "        shuffle=True,\n",
    "        random_state=None,\n",
    "        balance=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_groups=n_groups,\n",
    "            coordinates=coordinates,\n",
    "            method=method,\n",
    "            max_distance=max_distance,\n",
    "            n_splits=n_splits,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\n",
    "                \"Number of splits must be >=2 for clusterKFold. Given {}.\".format(\n",
    "                    n_splits\n",
    "                )\n",
    "            )\n",
    "        self.test_size = test_size\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.balance = balance\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Generates integer indices corresponding to test sets.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Columns should be the easting and northing coordinates of data\n",
    "            points, respectively.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target variable for supervised learning problems. Always\n",
    "            ignored.\n",
    "        groups : array-like, with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set. Always ignored.\n",
    "        Yields\n",
    "        ------\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        labels = spatial_clusters(\n",
    "            n_groups=self.n_groups,\n",
    "            coordinates=self.coordinates,\n",
    "            method=self.method,\n",
    "            max_distance=self.max_distance,\n",
    "            **self.kwargs\n",
    "        )\n",
    "\n",
    "        cluster_ids = np.unique(labels)\n",
    "        if self.n_splits > cluster_ids.size:\n",
    "            raise ValueError(\n",
    "                \"Number of k-fold splits ({}) cannot be greater than the number of \"\n",
    "                \"clusters ({}). Either decrease n_splits or increase the number of \"\n",
    "                \"clusters.\".format(self.n_splits, cluster_ids.size)\n",
    "            )\n",
    "        if self.shuffle:\n",
    "            check_random_state(self.random_state).shuffle(cluster_ids)\n",
    "        if self.balance:\n",
    "            cluster_sizes = [np.isin(labels, i).sum() for i in cluster_ids]\n",
    "            try:\n",
    "                split_points = _partition_by_sum(cluster_sizes, parts=self.n_splits)\n",
    "                folds = np.split(np.arange(cluster_ids.size), split_points)\n",
    "            except ValueError:\n",
    "                warnings.warn(\n",
    "                    \"Could not balance folds to have approximately the same \"\n",
    "                    \"number of data points. Dividing into folds with equal \"\n",
    "                    \"number of clusters instead. Decreasing n_splits or increasing \"\n",
    "                    \"the number of clusters may help.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                folds = [i for _, i in KFold(n_splits=self.n_splits).split(cluster_ids)]\n",
    "        else:\n",
    "            folds = [i for _, i in KFold(n_splits=self.n_splits).split(cluster_ids)]\n",
    "        for test_clusters in folds:\n",
    "            test_points = np.where(np.isin(labels, cluster_ids[test_clusters]))[0]\n",
    "            yield test_points\n",
    "            \n",
    "## dea_bandindices.py\n",
    "'''\n",
    "Description: This file contains a set of python functions for computing\n",
    "remote sensing band indices on Digital Earth Australia data.\n",
    "\n",
    "License: The code in this notebook is licensed under the Apache License,\n",
    "Version 2.0 (https://www.apache.org/licenses/LICENSE-2.0). Digital Earth\n",
    "Australia data is licensed under the Creative Commons by Attribution 4.0\n",
    "license (https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Contact: If you need assistance, please post a question on the Open Data\n",
    "Cube Slack channel (http://slack.opendatacube.org/) or on the GIS Stack\n",
    "Exchange (https://gis.stackexchange.com/questions/ask?tags=open-data-cube)\n",
    "using the `open-data-cube` tag (you can view previously asked questions\n",
    "here: https://gis.stackexchange.com/questions/tagged/open-data-cube).\n",
    "\n",
    "If you would like to report an issue with this script, you can file one\n",
    "on Github (https://github.com/GeoscienceAustralia/dea-notebooks/issues/new).\n",
    "\n",
    "Last modified: March 2021\n",
    "\n",
    "'''\n",
    "\n",
    "# Import required packages\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Define custom functions\n",
    "def calculate_indices(ds,\n",
    "                      index=None,\n",
    "                      collection=None,\n",
    "                      custom_varname=None,\n",
    "                      normalise=True,\n",
    "                      drop=False,\n",
    "                      inplace=False):\n",
    "    \"\"\"\n",
    "    Takes an xarray dataset containing spectral bands, calculates one of\n",
    "    a set of remote sensing indices, and adds the resulting array as a \n",
    "    new variable in the original dataset.  \n",
    "    \n",
    "    Note: by default, this function will create a new copy of the data\n",
    "    in memory. This can be a memory-expensive operation, so to avoid\n",
    "    this, set `inplace=True`.\n",
    "\n",
    "    Last modified: March 2021\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray Dataset\n",
    "        A two-dimensional or multi-dimensional array with containing the\n",
    "        spectral bands required to calculate the index. These bands are\n",
    "        used as inputs to calculate the selected water index.\n",
    "    index : str or list of strs\n",
    "        A string giving the name of the index to calculate or a list of\n",
    "        strings giving the names of the indices to calculate:\n",
    "        'AWEI_ns (Automated Water Extraction Index,\n",
    "                  no shadows, Feyisa 2014)\n",
    "        'AWEI_sh' (Automated Water Extraction Index,\n",
    "                   shadows, Feyisa 2014)\n",
    "        'BAEI' (Built-Up Area Extraction Index, Bouzekri et al. 2015)\n",
    "        'BAI' (Burn Area Index, Martin 1998)\n",
    "        'BSI' (Bare Soil Index, Rikimaru et al. 2002)\n",
    "        'BUI' (Built-Up Index, He et al. 2010)\n",
    "        'CMR' (Clay Minerals Ratio, Drury 1987)\n",
    "        'EVI' (Enhanced Vegetation Index, Huete 2002)\n",
    "        'FMR' (Ferrous Minerals Ratio, Segal 1982)\n",
    "        'IOR' (Iron Oxide Ratio, Segal 1982)\n",
    "        'LAI' (Leaf Area Index, Boegh 2002)\n",
    "        'MNDWI' (Modified Normalised Difference Water Index, Xu 1996)\n",
    "        'MSAVI' (Modified Soil Adjusted Vegetation Index,\n",
    "                 Qi et al. 1994)              \n",
    "        'NBI' (New Built-Up Index, Jieli et al. 2010)\n",
    "        'NBR' (Normalised Burn Ratio, Lopez Garcia 1991)\n",
    "        'NDBI' (Normalised Difference Built-Up Index, Zha 2003)\n",
    "        'NDCI' (Normalised Difference Chlorophyll Index, \n",
    "                Mishra & Mishra, 2012)\n",
    "        'NDMI' (Normalised Difference Moisture Index, Gao 1996)        \n",
    "        'NDSI' (Normalised Difference Snow Index, Hall 1995)\n",
    "        'NDTI' (Normalise Difference Tillage Index,\n",
    "                Van Deventeret et al. 1997)\n",
    "        'NDVI' (Normalised Difference Vegetation Index, Rouse 1973)\n",
    "        'NDWI' (Normalised Difference Water Index, McFeeters 1996)\n",
    "        'SAVI' (Soil Adjusted Vegetation Index, Huete 1988)\n",
    "        'TCB' (Tasseled Cap Brightness, Crist 1985)\n",
    "        'TCG' (Tasseled Cap Greeness, Crist 1985)\n",
    "        'TCW' (Tasseled Cap Wetness, Crist 1985)\n",
    "        'WI' (Water Index, Fisher 2016)\n",
    "        'kNDVI' (Non-linear Normalised Difference Vegation Index,\n",
    "                 Camps-Valls et al. 2021)\n",
    "    collection : str\n",
    "        An string that tells the function what data collection is \n",
    "        being used to calculate the index. This is necessary because \n",
    "        different collections use different names for bands covering \n",
    "        a similar spectra. Valid options are 'ga_ls_2' (for GA \n",
    "        Landsat Collection 2), 'ga_ls_3' (for GA Landsat Collection 3) \n",
    "        and 'ga_s2_1' (for GA Sentinel 2 Collection 1).\n",
    "    custom_varname : str, optional\n",
    "        By default, the original dataset will be returned with \n",
    "        a new index variable named after `index` (e.g. 'NDVI'). To \n",
    "        specify a custom name instead, you can supply e.g. \n",
    "        `custom_varname='custom_name'`. Defaults to None, which uses\n",
    "        `index` to name the variable. \n",
    "    normalise : bool, optional\n",
    "        Some coefficient-based indices (e.g. 'WI', 'BAEI', 'AWEI_ns', \n",
    "        'AWEI_sh', 'TCW', 'TCG', 'TCB', 'EVI', 'LAI', 'SAVI', 'MSAVI') \n",
    "        produce different results if surface reflectance values are not \n",
    "        scaled between 0.0 and 1.0 prior to calculating the index. \n",
    "        Setting `normalise=True` first scales values to a 0.0-1.0 range\n",
    "        by dividing by 10000.0. Defaults to True.  \n",
    "    drop : bool, optional\n",
    "        Provides the option to drop the original input data, thus saving \n",
    "        space. if drop = True, returns only the index and its values.\n",
    "    inplace: bool, optional\n",
    "        If `inplace=True`, calculate_indices will modify the original\n",
    "        array in-place, adding bands to the input dataset. The default\n",
    "        is `inplace=False`, which will instead make a new copy of the\n",
    "        original data (and use twice the memory).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ds : xarray Dataset\n",
    "        The original xarray Dataset inputted into the function, with a \n",
    "        new varible containing the remote sensing index as a DataArray.\n",
    "        If drop = True, the new variable/s as DataArrays in the \n",
    "        original Dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set ds equal to a copy of itself in order to prevent the function \n",
    "    # from editing the input dataset. This can prevent unexpected\n",
    "    # behaviour though it uses twice as much memory.    \n",
    "    if not inplace:\n",
    "        ds = ds.copy(deep=True)\n",
    "    \n",
    "    # Capture input band names in order to drop these if drop=True\n",
    "    if drop:\n",
    "        bands_to_drop=list(ds.data_vars)\n",
    "        print(f'Dropping bands {bands_to_drop}')\n",
    "\n",
    "    # Dictionary containing remote sensing index band recipes\n",
    "    index_dict = {\n",
    "                  # Normalised Difference Vegation Index, Rouse 1973\n",
    "                  'NDVI': lambda ds: (ds.nir - ds.red) /\n",
    "                                     (ds.nir + ds.red),\n",
    "        \n",
    "                  # NDVI, after Rouse 1973 using Sentinel-2 8A band\n",
    "                  'NDVI8a': lambda ds: (ds.narrow_nir - ds.red) /\n",
    "                                     (ds.narrow_nir + ds.red),\n",
    "        \n",
    "                  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                  # Non-linear Normalised Difference Vegation Index,\n",
    "                  # Camps-Valls et al. 2021\n",
    "                  'kNDVI': lambda ds: np.tanh(((ds.nir - ds.red) /\n",
    "                                               (ds.nir + ds.red)) ** 2),\n",
    "\n",
    "                  # Enhanced Vegetation Index, Huete 2002\n",
    "                  'EVI': lambda ds: ((2.5 * (ds.nir - ds.red)) /\n",
    "                                     (ds.nir + 6 * ds.red -\n",
    "                                      7.5 * ds.blue + 1)),\n",
    "\n",
    "                  # Leaf Area Index, Boegh 2002\n",
    "                  'LAI': lambda ds: (3.618 * ((2.5 * (ds.nir - ds.red)) /\n",
    "                                     (ds.nir + 6 * ds.red -\n",
    "                                      7.5 * ds.blue + 1)) - 0.118),\n",
    "\n",
    "                  # Soil Adjusted Vegetation Index, Huete 1988\n",
    "                  'SAVI': lambda ds: ((1.5 * (ds.nir - ds.red)) /\n",
    "                                      (ds.nir + ds.red + 0.5)),\n",
    "      \n",
    "                  # Mod. Soil Adjusted Vegetation Index, Qi et al. 1994\n",
    "                  'MSAVI': lambda ds: ((2 * ds.nir + 1 - \n",
    "                                      ((2 * ds.nir + 1)**2 - \n",
    "                                       8 * (ds.nir - ds.red))**0.5) / 2),    \n",
    "\n",
    "                  # Normalised Difference Moisture Index, Gao 1996\n",
    "                  'NDMI': lambda ds: (ds.nir - ds.swir1) /\n",
    "                                     (ds.nir + ds.swir1),\n",
    "\n",
    "                  # Normalised Burn Ratio, Lopez Garcia 1991\n",
    "                  'NBR': lambda ds: (ds.nir - ds.swir2) /\n",
    "                                    (ds.nir + ds.swir2),\n",
    "\n",
    "                  # Burn Area Index, Martin 1998\n",
    "                  'BAI': lambda ds: (1.0 / ((0.10 - ds.red) ** 2 +\n",
    "                                            (0.06 - ds.nir) ** 2)),\n",
    "        \n",
    "                 # Normalised Difference Chlorophyll Index, \n",
    "                 # (Mishra & Mishra, 2012)\n",
    "                  'NDCI': lambda ds: (ds.red_edge1 - ds.red) /\n",
    "                                     (ds.red_edge1 + ds.red),\n",
    "\n",
    "                  # Normalised Difference Snow Index, Hall 1995\n",
    "                  'NDSI': lambda ds: (ds.green - ds.swir1) /\n",
    "                                     (ds.green + ds.swir1),\n",
    "\n",
    "                  # Normalised Difference Tillage Index,\n",
    "                  # Van Deventer et al. 1997\n",
    "                  'NDTI': lambda ds: (ds.swir1 - ds.swir2) /\n",
    "                                     (ds.swir1 + ds.swir2),\n",
    "\n",
    "                  # Normalised Difference Water Index, McFeeters 1996\n",
    "                  'NDWI': lambda ds: (ds.green - ds.nir) /\n",
    "                                     (ds.green + ds.nir),\n",
    "\n",
    "                  # Modified Normalised Difference Water Index, Xu 2006\n",
    "                  'MNDWI': lambda ds: (ds.green - ds.swir1) /\n",
    "                                      (ds.green + ds.swir1),\n",
    "      \n",
    "                  # Normalised Difference Built-Up Index, Zha 2003\n",
    "                  'NDBI': lambda ds: (ds.swir1 - ds.nir) /\n",
    "                                     (ds.swir1 + ds.nir),\n",
    "      \n",
    "                  # Built-Up Index, He et al. 2010\n",
    "                  'BUI': lambda ds:  ((ds.swir1 - ds.nir) /\n",
    "                                      (ds.swir1 + ds.nir)) -\n",
    "                                     ((ds.nir - ds.red) /\n",
    "                                      (ds.nir + ds.red)),\n",
    "      \n",
    "                  # Built-up Area Extraction Index, Bouzekri et al. 2015\n",
    "                  'BAEI': lambda ds: (ds.red + 0.3) /\n",
    "                                     (ds.green + ds.swir1),\n",
    "      \n",
    "                  # New Built-up Index, Jieli et al. 2010\n",
    "                  'NBI': lambda ds: (ds.swir1 + ds.red) / ds.nir,\n",
    "      \n",
    "                  # Bare Soil Index, Rikimaru et al. 2002\n",
    "                  'BSI': lambda ds: ((ds.swir1 + ds.red) - \n",
    "                                     (ds.nir + ds.blue)) / \n",
    "                                    ((ds.swir1 + ds.red) + \n",
    "                                     (ds.nir + ds.blue)),\n",
    "\n",
    "                  # Automated Water Extraction Index (no shadows), Feyisa 2014\n",
    "                  'AWEI_ns': lambda ds: (4 * (ds.green - ds.swir1) -\n",
    "                                        (0.25 * ds.nir * + 2.75 * ds.swir2)),\n",
    "\n",
    "                  # Automated Water Extraction Index (shadows), Feyisa 2014\n",
    "                  'AWEI_sh': lambda ds: (ds.blue + 2.5 * ds.green -\n",
    "                                         1.5 * (ds.nir + ds.swir1) -\n",
    "                                         0.25 * ds.swir2),\n",
    "\n",
    "                  # Water Index, Fisher 2016\n",
    "                  'WI': lambda ds: (1.7204 + 171 * ds.green + 3 * ds.red -\n",
    "                                    70 * ds.nir - 45 * ds.swir1 -\n",
    "                                    71 * ds.swir2),\n",
    "        ## Tasseled Cap\n",
    "                  # Tasseled Cap Transformations are influenced by the sensor\n",
    "                  # I.e. Crist (1985) converted MSS (after Kauth and Thoma, 1976)\n",
    "                  # values to TM values which are used here:\n",
    "        \n",
    "                  # Tasseled Cap Wetness, Crist 1985\n",
    "                  'TCW': lambda ds: (0.0315 * ds.blue + 0.2021 * ds.green +\n",
    "                                     0.3102 * ds.red + 0.1594 * ds.nir +\n",
    "                                    -0.6806 * ds.swir1 + -0.6109 * ds.swir2),\n",
    "\n",
    "                  # Tasseled Cap Greeness, Crist 1985\n",
    "                  'TCG': lambda ds: (-0.1603 * ds.blue + -0.2819 * ds.green +\n",
    "                                     -0.4934 * ds.red + 0.7940 * ds.nir +\n",
    "                                     -0.0002 * ds.swir1 + -0.1446 * ds.swir2),\n",
    "\n",
    "                  # Tasseled Cap Brightness, Crist 1985\n",
    "                  'TCB': lambda ds: (0.2043 * ds.blue + 0.4158 * ds.green +\n",
    "                                     0.5524 * ds.red + 0.5741 * ds.nir +\n",
    "                                     0.3124 * ds.swir1 + -0.2303 * ds.swir2),\n",
    "        \n",
    "                  # Tasseled Cap coefficients for Sentinel-2 are calculated using Gram-Schmidt orthogonalization (GSO)\n",
    "                  # by Nedkov, 2017 and using a Procrustes Analysis (PCP) by Shi, 2019, for values see csv-file:\n",
    "\n",
    "        \"\"\"Bands,\"GSO (Nedkov, 2017)\",,,\"PCP (Shi, 2019)\",,,,,,,,\n",
    ",B, G, W, B, G, W,,,,,,\n",
    "B1-Coastal,0.0356,-0.0635,0.0649,0.2381,-0.2266,0.1825,,,,,,\n",
    "B2-Blue,0.0822,-0.1128,0.1363,0.2569,-0.2818,0.1763,,,,,,\n",
    "B3-Green,0.136,-0.168,0.2802,0.2934,-0.302,0.1615,,,,,,\n",
    "B4-Red,0.2611,-0.348,0.3072,0.302,-0.4283,0.0486,,,,,,\n",
    "B5-RE-1,0.2964,-0.3303,0.5288,0.3099,-0.2959,0.017,,,,,, \n",
    "B6-RE-2,0.3338,0.0852,0.1379,0.374,0.1602,0.0223,,,,,,\n",
    "B7-RE-3,0.3877,0.3302,-0.0001,0.418,0.3127,0.0219,,,,,,\n",
    "B8-NIR-1,0.3895,0.3165,-0.0807,0.358,0.3138,-0.0755,,,,,,\n",
    "B8A-NIR-2,0.475,0.3625,-0.1389,0.3834,0.4261,-0.091,,,,,,\n",
    "B9-WV  ,0.0949,0.0467,-0.0302,0.0103,0.1454,-0.1369,,,,,,\n",
    "B10-Cirrus,0.0009,-0.0009,0.0003,0.002,-0.0017,0.0003,,,,,,\n",
    "B11-SWIR-1,0.3882,-0.4587,-0.4064,0.0896,-0.1341,-0.771,,,,,,\n",
    "B12-SWIR-2,0.1366,-0.4064,-0.5602,0.078,-0.2538,-0.5293,,,,,,\n",
    "\"\"\"\n",
    "        \n",
    "                 # Tasseled Cap Wetness, GSO after Nedkov\n",
    "                  'TCW_GSO': lambda ds: (0.0649 * ds.blue + 0.2802 * ds.green +\n",
    "                                     0.3072 * ds.red + -0.0807 * ds.nir +\n",
    "                                    -0.4064 * ds.swir1 + -0.5602 * ds.swir2),\n",
    "\n",
    "                  # Tasseled Cap Greeness, GSO\n",
    "                  'TCG_GSO': lambda ds: (-0.0635 * ds.blue + -0.168 * ds.green +\n",
    "                                     -0.348 * ds.red + 0.3895 * ds.nir +\n",
    "                                     -0.4587 * ds.swir1 + -0.4064 * ds.swir2),\n",
    "\n",
    "                  # Tasseled Cap Brightness, GSO\n",
    "                  'TCB_GSO': lambda ds: (0.0822 * ds.blue + 0.136 * ds.green +\n",
    "                                     0.2611 * ds.red + 0.5741 * ds.nir +\n",
    "                                     0.3882 * ds.swir1 + 0.1366 * ds.swir2),\n",
    "        \n",
    "        \n",
    "                  # Clay Minerals Ratio, Drury 1987\n",
    "                  'CMR': lambda ds: (ds.swir1 / ds.swir2),\n",
    "\n",
    "                  # Ferrous Minerals Ratio, Segal 1982\n",
    "                  'FMR': lambda ds: (ds.swir1 / ds.nir),\n",
    "\n",
    "                  # Iron Oxide Ratio, Segal 1982\n",
    "                  'IOR': lambda ds: (ds.red / ds.blue)\n",
    "    }\n",
    "    \n",
    "    # If index supplied is not a list, convert to list. This allows us to\n",
    "    # iterate through either multiple or single indices in the loop below\n",
    "    indices = index if isinstance(index, list) else [index]\n",
    "    \n",
    "    #calculate for each index in the list of indices supplied (indexes)\n",
    "    for index in indices:\n",
    "\n",
    "        # Select an index function from the dictionary\n",
    "        index_func = index_dict.get(str(index))\n",
    "\n",
    "        # If no index is provided or if no function is returned due to an \n",
    "        # invalid option being provided, raise an exception informing user to \n",
    "        # choose from the list of valid options\n",
    "        if index is None:\n",
    "\n",
    "            raise ValueError(f\"No remote sensing `index` was provided. Please \"\n",
    "                              \"refer to the function \\ndocumentation for a full \"\n",
    "                              \"list of valid options for `index` (e.g. 'NDVI')\")\n",
    "\n",
    "        elif (index in ['WI', 'BAEI', 'AWEI_ns', 'AWEI_sh', 'TCW', \n",
    "                        'TCG', 'TCB', 'EVI', 'LAI', 'SAVI', 'MSAVI'] \n",
    "              and not normalise):\n",
    "\n",
    "            warnings.warn(f\"\\nA coefficient-based index ('{index}') normally \"\n",
    "                           \"applied to surface reflectance values in the \\n\"\n",
    "                           \"0.0-1.0 range was applied to values in the 0-10000 \"\n",
    "                           \"range. This can produce unexpected results; \\nif \"\n",
    "                           \"required, resolve this by setting `normalise=True`\")\n",
    "\n",
    "        elif index_func is None:\n",
    "\n",
    "            raise ValueError(f\"The selected index '{index}' is not one of the \"\n",
    "                              \"valid remote sensing index options. \\nPlease \"\n",
    "                              \"refer to the function documentation for a full \"\n",
    "                              \"list of valid options for `index`\")\n",
    "\n",
    "        # Rename bands to a consistent format if depending on what collection\n",
    "        # is specified in `collection`. This allows the same index calculations\n",
    "        # to be applied to all collections. If no collection was provided, \n",
    "        # raise an exception.\n",
    "        if collection is None:\n",
    "\n",
    "            raise ValueError(\"'No `collection` was provided. Please specify \"\n",
    "                             \"either 'ga_ls_2', 'ga_ls_3' or 'ga_s2_1' \\nto \"\n",
    "                             \"ensure the function calculates indices using the \"\n",
    "                             \"correct spectral bands\")\n",
    "\n",
    "        elif collection == 'ga_ls_3':\n",
    "\n",
    "            # Dictionary mapping full data names to simpler 'red' alias names\n",
    "            bandnames_dict = {\n",
    "                'nbart_nir': 'nir',\n",
    "                'nbart_red': 'red',\n",
    "                'nbart_green': 'green',\n",
    "                'nbart_blue': 'blue',\n",
    "                'nbart_swir_1': 'swir1',\n",
    "                'nbart_swir_2': 'swir2',\n",
    "                'nbar_red': 'red',\n",
    "                'nbar_green': 'green',\n",
    "                'nbar_blue': 'blue',\n",
    "                'nbar_nir': 'nir',\n",
    "                'nbar_swir_1': 'swir1',\n",
    "                'nbar_swir_2': 'swir2'\n",
    "            }\n",
    "\n",
    "            # Rename bands in dataset to use simple names (e.g. 'red')\n",
    "            bands_to_rename = {\n",
    "                a: b for a, b in bandnames_dict.items() if a in ds.variables\n",
    "            }\n",
    "\n",
    "        elif collection == 'ga_s2_1':\n",
    "\n",
    "            # Dictionary mapping full data names to simpler 'red' alias names\n",
    "            bandnames_dict = {\n",
    "                'nbart_red': 'red',\n",
    "                'nbart_green': 'green',\n",
    "                'nbart_blue': 'blue',\n",
    "                'nbart_nir_1': 'nir',\n",
    "                'nbart_red_edge_1': 'red_edge_1', \n",
    "                'nbart_red_edge_2': 'red_edge_2',    \n",
    "                'nbart_swir_2': 'swir1',\n",
    "                'nbart_swir_3': 'swir2',\n",
    "                'nbar_red': 'red',\n",
    "                'nbar_green': 'green',\n",
    "                'nbar_blue': 'blue',\n",
    "                'nbar_nir_1': 'nir',\n",
    "                'nbar_red_edge_1': 'red_edge_1',   \n",
    "                'nbar_red_edge_2': 'red_edge_2',   \n",
    "                'nbar_swir_2': 'swir1',\n",
    "                'nbar_swir_3': 'swir2'\n",
    "            }\n",
    "\n",
    "            # Rename bands in dataset to use simple names (e.g. 'red')\n",
    "            bands_to_rename = {\n",
    "                a: b for a, b in bandnames_dict.items() if a in ds.variables\n",
    "            }\n",
    "\n",
    "        elif collection == 'ga_ls_2':\n",
    "\n",
    "            # Pass an empty dict as no bands need renaming\n",
    "            bands_to_rename = {}\n",
    "\n",
    "        # Raise error if no valid collection name is provided:\n",
    "        else:\n",
    "            raise ValueError(f\"'{collection}' is not a valid option for \"\n",
    "                              \"`collection`. Please specify either \\n\"\n",
    "                              \"'ga_ls_2', 'ga_ls_3' or 'ga_s2_1'\")\n",
    "\n",
    "        # Apply index function \n",
    "        try:\n",
    "            # If normalised=True, divide data by 10,000 before applying func\n",
    "            mult = 10000.0 if normalise else 1.0\n",
    "            index_array = index_func(ds.rename(bands_to_rename) / mult)\n",
    "        except AttributeError:\n",
    "            raise ValueError(f'Please verify that all bands required to '\n",
    "                             f'compute {index} are present in `ds`. \\n'\n",
    "                             f'These bands may vary depending on the `collection` '\n",
    "                             f'(e.g. the Landsat `nbart_nir` band \\n'\n",
    "                             f'is equivelent to `nbart_nir_1` for Sentinel 2)')\n",
    "\n",
    "        # Add as a new variable in dataset\n",
    "        output_band_name = custom_varname if custom_varname else index\n",
    "        ds[output_band_name] = index_array\n",
    "    \n",
    "    # Once all indexes are calculated, drop input bands if inplace=False\n",
    "    if drop and not inplace:\n",
    "        ds = ds.drop(bands_to_drop)\n",
    "\n",
    "    # If inplace == True, delete bands in-place instead of using drop\n",
    "    if drop and inplace:\n",
    "        for band_to_drop in bands_to_drop:\n",
    "            del ds[band_to_drop]\n",
    "\n",
    "    # Return input dataset with added water index variable\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14c589-177d-458a-aa8e-79ef322b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# really needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "from odc.stac import stac_load\n",
    "from datacube.utils.cog import write_cog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed65e9f-731a-4098-aab2-d9188f2dd292",
   "metadata": {},
   "source": [
    "# **Reproducing results using original data, oversampling and undersampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c114227-3d78-45f8-85d3-76f4e63b8156",
   "metadata": {},
   "source": [
    "Read from already restructured csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153de0cc-5eab-46fd-b0bc-919ad5a2dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tables and bring them in shape\n",
    "# \n",
    "training_data = 'extract/2019-04_mspc.csv'\n",
    "#training_data = '2019_predictors_veg_per.csv'\n",
    "\n",
    "df = pd.read_table(training_data, sep=',')\n",
    "df = df.drop(['x_coord', 'y_coord'], axis=1)\n",
    "#df = df.drop('fid', axis=1)\n",
    "\n",
    "model_variables = df.drop('class', axis=1).columns.values.tolist()\n",
    "column_names = model_variables\n",
    "y = df['class']\n",
    "X = df.drop('class', axis=1)\n",
    "\n",
    "#training_data = '2019_S2_predictors.csv'\n",
    "#training_data = '2019_predictors_veg_per.csv'\n",
    "\n",
    "#df = pd.read_table(training_data, sep=',')\n",
    "# clear residuals\n",
    "# for non balanced classification\n",
    "#df = df.drop(['Unnamed: 0', 'class2', 'class5', 'class6', 'class9', 'class10', 'class13', 'class14', 'class17', 'class18', 'class21', 'class22', 'y_coord','x_coord'], axis=1)\n",
    "# for balanced classification\n",
    "#df = df.drop(['Unnamed: 0','class.1'], axis=1)\n",
    "#df = df.drop('fid', axis=1)\n",
    "\n",
    "#model_variables = df.drop('class', axis=1).columns.values.tolist()\n",
    "#column_names = model_variables\n",
    "#y = df['class']\n",
    "#X = df.drop('class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a863a7-8972-401c-8382-fb4b37ab47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557dce8-7294-47ed-900a-ae5876f03071",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c6a1c-1b17-4a80-b73d-82b2c5317114",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Not balanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be99bea-db7e-4f24-9d68-960665c9430e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X = X.drop(['scl', 'x_coord', 'y_coord'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred_test = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7638803-7497-4b6b-aca1-0f716c53aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0da78-864f-41dc-8eef-1cf723205bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c2c9c-c765-40a6-b5ad-55af0195cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred_test)\n",
    "matrix = matrix.astype('float')# / matrix.sum(axis=1)[:, np.newaxis] *100\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(20,7))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Ash','Beech','Oak','OBL','Hardwood','Birch','Alder','Poplar','Larch','Pine','Spruce', 'OCL']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95830872-4f8c-448a-8599-aa9d65070704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the feature importance of the input features for predicting the class labels provided\n",
    "model = forest\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "plt.subplots(figsize=(70,20))\n",
    "plt.gcf().subplots_adjust(bottom=0.5)\n",
    "plt.bar(x=model_variables, yerr=std, height=model.feature_importances_)\n",
    "\n",
    "\n",
    "plt.gca().set_ylabel('Importance', labelpad=10,size=15)\n",
    "plt.gca().set_xlabel('Variable', labelpad=10,size=15)\n",
    "plt.tick_params(axis='y',labelsize=20)\n",
    "plt.tick_params(axis='x', rotation=90,labelsize=20)\n",
    "#plt.savefig('2019_all_vi.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f03e9f-83cd-4f9d-9fa0-2b8e5b1d7d64",
   "metadata": {},
   "source": [
    "## **SVM SMOTE**\n",
    "Separate train and test data and balancing with svm smote for training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b29c5-92ac-4e61-bac5-bcd64823dbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train-test-split vor balancing\n",
    "# zuvor wurden die Ergebnisse dem random forest schon gezeigt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "# resample train and test data using svw-smote\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "X_train1, y_train1 = sm.fit_resample(X_train, y_train)\n",
    "X_res, y_res = sm.fit_resample(X_train1, y_train1)\n",
    "X_test1, y_test1 = sm.fit_resample(X_test, y_test)# check if balanced\n",
    "X_test2, y_test2 = sm.fit_resample(X_test1, y_test1)\n",
    "X_test3, y_test3 = sm.fit_resample(X_test2, y_test2)\n",
    "X_res_test, y_res_test = sm.fit_resample(X_test3, y_test3)\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord']#,'scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927b459-1345-48c7-9af7-76f435461677",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48ec77-595e-4570-b5b8-b50d103501e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fd76a-5783-4274-93b9-a75e56519861",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee37eb-5a0b-4139-83b1-781c5fdb8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214edb1-985e-4575-8fb9-072fdf1e5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cf19b-18e1-42f8-a0d8-501eb250d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b8c1f-82c1-4213-a2a8-713142ccf012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49d07e-37b4-4a92-ae10-9ff54282e246",
   "metadata": {},
   "source": [
    "## **SMOTE**\n",
    "Separate train and test data and balancing with smote for training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc05942-4758-409f-99a6-5568c56da06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# train-test-split vor balancing\n",
    "# zuvor wurden die Ergebnisse dem random forest schon gezeigt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "# resample train and test data using svw-smote\n",
    "smb = SMOTE(random_state=42)\n",
    "X_train1, y_train1 = smb.fit_resample(X_train, y_train) # one time extra resample balance equal\n",
    "X_res, y_res = smb.fit_resample(X_train1, y_train1)\n",
    "X_res_test, y_res_test = smb.fit_resample(X_test, y_test) # one time for test enough\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a9ade-b3ff-40e2-a10b-ddcb749eeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893a7a5-eb51-4ed1-846d-ddce9f0795f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7208d-9063-4018-a35f-2ff7e1ae6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272e721-8c3b-41e9-9e3c-5dd3d5f507a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ef664-f9b0-4f6d-a550-04978b765447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cad12-f3aa-4026-90e2-ea8332a1fec2",
   "metadata": {},
   "source": [
    "## **ADASYN**\n",
    "Separate train and test data and balancing with ADASYN for training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cf158-0c17-4795-a8c6-1c18b6f7306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "# train-test-split vor balancing\n",
    "# zuvor wurden die Ergebnisse dem random forest schon gezeigt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "# resample train and test data using svw-smote\n",
    "\n",
    "adsy = ADASYN(random_state=42)\n",
    "X_res, y_res = adsy.fit_resample(X_train, y_train)\n",
    "X_res_test, y_res_test = adsy.fit_resample(X_test, y_test)\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord','scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782df6eb-2899-4084-93c7-c38eef1f99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd86f90-ae0f-43d4-a476-94186db46379",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531497a-4765-4c82-bc48-3033b3925fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33ed90-b705-4292-bee5-6658e6fe8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e313f-e5af-4eee-9cc9-c5a04033d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152b0c4-c5b4-410d-a514-e619f5001321",
   "metadata": {},
   "source": [
    "## **Borderline SMOTE**\n",
    "Separate train and test data and balancing with borderline smote for training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca0bcf-070f-4acc-b944-2b32ff0e63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE# train-test-split vor balancing\n",
    "# zuvor wurden die Ergebnisse dem random forest schon gezeigt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "# resample train and test data using svw-smote\n",
    "\n",
    "bsm = BorderlineSMOTE(random_state=42)\n",
    "X_res, y_res = bsm.fit_resample(X_train, y_train)\n",
    "X_res_test, y_res_test = bsm.fit_resample(X_test, y_test)\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord','scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa609bd-cb1f-403e-9a23-69d9254e41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34193104-0306-464d-a81b-b84c78beb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42bb93-d026-494e-85a2-756473a38b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a792f-65a5-4def-8517-b243e0ae1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16462ed-12de-4415-9add-af6d0a2fbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff433920-dca8-439d-88a3-dbe2917bd06f",
   "metadata": {},
   "source": [
    "## **Random oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b21799-c008-4486-b971-e9044bebe9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "\n",
    "rus = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "X_res_test, y_res_test = rus.fit_resample(X_test, y_test)\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord','scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612c7cd-0294-412d-ab69-6506eb628e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9c252-4fc8-4765-8913-425394012c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15c035-3935-4260-8c7a-3d3b7114b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ab410-bd88-478a-97f4-0ba67566fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932338bf-6742-4d4c-bbca-a94c2357d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a15e2a-0e8c-493e-b56a-47cc43d00355",
   "metadata": {},
   "source": [
    "## **Random undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d757a-6c63-4a20-92bc-ed1629922c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "X_res_test, y_res_test = rus.fit_resample(X_test, y_test)\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord','scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3abb56-ef12-4f14-a214-e2c7b5c52aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5590b-31cd-4622-9224-c18f6d340e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494ebcf-7f31-45e7-9f42-7bd7d4badcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016c10b-fcd5-420c-bf39-ca545f1f9ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee64bbb-e7a4-4638-a997-7870de67f915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e4b5c-1389-4780-9204-dec77ac3832a",
   "metadata": {},
   "source": [
    "## **Condensed nearest neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb0a2c-0dba-457b-9892-6335f03e2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.25, stratify=y)\n",
    "\n",
    "cnn = CondensedNearestNeighbour(random_state=42)\n",
    "X_res, y_res = cnn.fit_resample(X_train, y_train)\n",
    "X_res_test, y_res_test = cnn.fit_resample(X_test, y_test)\n",
    "\n",
    "#\n",
    "weg_muss = ['y_coord','x_coord','scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "X_res_test = X_res_test.drop(columns=weg_muss)\n",
    "y_res_test = y_res_test.drop(columns=weg_muss)\n",
    "\n",
    "model_variables = X_res.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264f9e4-51e8-496d-b4b3-7b3c54c262c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res_test.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daeec4-d658-4e1c-b7bb-08be0352d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f24ce8-66d3-41a6-a253-aba2927a4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=1000,n_jobs=30,random_state=42,class_weight='balanced')\n",
    "forest.fit(X_res, y_res)\n",
    "y_pred_test = forest.predict(X_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30378043-96db-40f2-954f-728bb75c9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_res_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a06df7-4f48-40a6-bbb2-9f67af9f3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "matrix = confusion_matrix(y_res_test, y_pred_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':20},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Beech','Birch','Oak','Alder','Spruce','Pine','Larch']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CM_2019-04_median_balanced.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eabb3dbf-1501-4966-bf1c-77f7aa6d8cde",
   "metadata": {},
   "source": [
    "# FIRST ACCEPTABLE RESULTS\n",
    "# issue with not equaly balancd data exist further --> repeated\n",
    "# balancing using coordinates affects results --> \n",
    "\n",
    "# balance via smote erste verfahren, nach ESA 2022\n",
    "# einmaliges balancing führte für klasse 3 zu ~ 4000 prediktoren anstatt ~10000\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "#,sampling_strategy={4: 12000, 0: 12000, 5: 12000, 2: 12000, 1: 12000, 3: 12000, 6: 12000}) # majority <=\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "#\n",
    "# delete predictors\n",
    "weg_muss = ['y_coord','x_coord']#,'scl']\n",
    "X_res = X_res.drop(columns=weg_muss)\n",
    "y_res = y_res.drop(columns=weg_muss)\n",
    "model_variables = X_res.columns.values.tolist()\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state=42, stratify=y_res)\n",
    "# choose model\n",
    "model = RandomForestClassifier(n_estimators=1000,n_jobs=30, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79754f-0f21-4cb1-98ce-f4aa9a8b1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second resample\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "#,sampling_strategy={4: 12000, 0: 12000, 5: 12000, 2: 12000, 1: 12000, 3: 12000, 6: 12000}) # majority <=\n",
    "X_res01, y_res01 = sm.fit_resample(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84d85a-12e7-43be-8d09-aaa98de07fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res01, y_res01, random_state=42, stratify=y_res01)\n",
    "# choose model\n",
    "model = RandomForestClassifier(n_estimators=1000,n_jobs=30, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bc83d-c2ae-4523-b602-d055c31f3df7",
   "metadata": {},
   "source": [
    "## Predict data\n",
    "Using monthly products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fb661-667e-4409-88b9-c8a6a8d4caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC data using instead of local\n",
    "\n",
    "x_min = 8\n",
    "x_max = 15\n",
    "y_min = 47\n",
    "y_max = 51\n",
    "\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [ \n",
    "        [\n",
    "            [x_max, y_min],\n",
    "            [x_min, y_min],\n",
    "            [x_min, y_max],\n",
    "            [x_max, y_max],\n",
    "            [x_max, y_min],\n",
    "        ]\n",
    "    ],}\n",
    "# [xmax,ymin],[xmin,ymin],[xmin,ymax],[xmax,ymax],[xmax,ymin]\n",
    "catalog = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace)\n",
    "\n",
    "query = {'output_crs' : 'EPSG:25832'}\n",
    "\n",
    "time_of_interest = \"2019-04\"\n",
    "search = catalog.search(collections=[\"sentinel-2-l2a\"],\n",
    "                        intersects=area_of_interest,\n",
    "                        datetime=time_of_interest,\n",
    "                        #query={\"eo:cloud_cover\": {\"lt\": 100}},\n",
    ")\n",
    "items = search.item_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3331dd0-a066-4801-80a8-81959fb4e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using local data\n",
    "import gc\n",
    "easting_start= 447000 #+ (3 * 9950)\n",
    "northing_end=  5650000\n",
    "northing_start=5220000\n",
    "\n",
    "for zahl in range(0,1):\n",
    "    search = catalog.search(collections=[\"sentinel-2-l2a\"],\n",
    "                        intersects=area_of_interest,\n",
    "                        datetime=time_of_interest,\n",
    "                        #query={\"eo:cloud_cover\": {\"lt\": 100}},\n",
    "                           )\n",
    "    items = search.item_collection()\n",
    "    \n",
    "    ds05 = stac_load(groupby=\"solar_day\", \n",
    "                 items=items,\n",
    "                 x=[easting_start,easting_start+10000],\n",
    "                 y=[northing_start, northing_end], \n",
    "                 #chunks={}, \n",
    "                 **query)\n",
    "    ds05=ds05.where(ds05.SCL.isin([4, 5, 6, 7,11]))\n",
    "    ds05=ds05.drop_vars('SCL')\n",
    "    ds05 = ds05.median('time')\n",
    "    \n",
    "    # band wise renaming\n",
    "    ds05['coastal_aerosol'] = ds05['B01'] \n",
    "    ds05['blue'] = ds05['B02']\n",
    "    ds05['green'] = ds05['B03']\n",
    "    ds05['red'] = ds05['B04']\n",
    "    ds05['red_edge1'] = ds05['B05']\n",
    "    ds05['red_edge2'] = ds05['B06']\n",
    "    ds05['red_edge3'] = ds05['B07']\n",
    "    ds05['nir'] = ds05['B08']\n",
    "    ds05['narrow_nir'] = ds05['B8A']\n",
    "    ds05['water_vapour'] = ds05['B09']\n",
    "    ds05['swir1'] = ds05['B11']\n",
    "    ds05['swir2'] = ds05['B12']\n",
    "    \n",
    "    \n",
    "    ds05 = calculate_indices(ds05,index=['NDVI', 'kNDVI', 'NDVI8a', 'EVI', 'TCG_GSO','TCG','LAI','SAVI',\n",
    "                                 'MSAVI','BUI','NDBI','NDMI','BAEI','BSI'], collection='ga_s2_1')\n",
    "    \n",
    "    # band wise renaming\n",
    "    ds05['may_ca'] = ds05['coastal_aerosol']\n",
    "    ds05['may_b'] = ds05['blue']\n",
    "    ds05['may_g'] = ds05['green']\n",
    "    ds05['may_r'] = ds05['red']\n",
    "    ds05['may_r1'] = ds05['red_edge1']\n",
    "    ds05['may_r2'] = ds05['red_edge2']\n",
    "    ds05['may_r3'] = ds05['red_edge3']\n",
    "    ds05['may_n'] = ds05['nir'] \n",
    "    ds05['may_nn'] = ds05['narrow_nir']\n",
    "    ds05['may_wv'] = ds05['water_vapour']\n",
    "    ds05['may_s1'] = ds05['swir1']\n",
    "    ds05['may_s2'] = ds05['swir2']\n",
    "    # vi wise renaming\n",
    "    ds05['may_NDVI'] = ds05['NDVI']\n",
    "    ds05['may_kNDVI'] = ds05['kNDVI']\n",
    "    ds05['may_NDVI8a'] = ds05['NDVI8a']\n",
    "    ds05['may_EVI'] = ds05['EVI']\n",
    "    ds05['may_TCG_GSO'] = ds05['TCG_GSO']\n",
    "    ds05['may_TCG'] = ds05['TCG']\n",
    "    ds05['may_LAI'] = ds05['LAI']\n",
    "    ds05['may_SAVI'] = ds05['SAVI']\n",
    "    ds05['may_MSAVI'] = ds05['MSAVI']\n",
    "    ds05['may_BUI'] = ds05['BUI']\n",
    "    ds05['may_NDBI'] = ds05['NDBI']\n",
    "    ds05['may_NDMI'] = ds05['NDMI']\n",
    "    ds05['may_BAEI'] = ds05['BAEI']\n",
    "    ds05['may_BSI'] = ds05['BSI']\n",
    "    \n",
    "    ds05=ds05.drop_vars(['B01','B02','B03','B04','B05','B06','B07','B08','B09','B8A','B12','B11', \n",
    "            'coastal_aerosol','blue','green','red','red_edge1','red_edge2','red_edge3', \n",
    "            'nir','narrow_nir','water_vapour','swir1','swir2', 'AOT', 'visual', 'WVP', \n",
    "            'NDVI', 'kNDVI', 'NDVI8a', 'EVI', 'TCG_GSO','TCG','LAI','SAVI','MSAVI','BUI', \n",
    "            'NDBI', 'NDMI','BAEI','BSI'\n",
    "            ])\n",
    "    \n",
    "    predicted = predict_xr(forest,ds05.compute(),clean=True,proba=True)\n",
    "    #predicted = predict_xr(forest,pred_comp,clean=True,proba=True)\n",
    "    #\n",
    "    ds=write_cog(geo_im=predicted.Predictions.astype('float64'),fname=(str(zahl)+'_2019-04_groundthruth_pred.tif'),overwrite=True,use_windowed_writes=True)\n",
    "    ds=write_cog(geo_im=predicted.Probabilities.astype('float64'),fname=(str(zahl)+'_2019-04_groundthruth_prob.tif'),overwrite=True,use_windowed_writes=True)\n",
    "    \n",
    "    del ds05\n",
    "    del predicted\n",
    "    del ds\n",
    "    gc.collect()\n",
    "    \n",
    "    easting_start=easting_start+9950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13fbd4-c623-4b22-9d5e-2105a5fca936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbc72a-e707-4831-b412-6fe9724d744d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
