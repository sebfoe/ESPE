{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest inventory\n",
    "\n",
    "* **Start Jupyter:** To run this notebook interactively from your browser\n",
    "\n",
    "### Background\n",
    "\n",
    "Forests are complex biomes. Anaylzing them is an important task to enhance the understanding, also in the context of the current global change. Forests have an important role, e.g. econmical or as carbon sink, which can also be conenected to monetary aspects (CO2 certificates). Furhter societal benefits are in the context of healthcare, e.g. allergies or considering upcoming zoonoses like the Covid-19 pandemic as well as forests role in recreation.\n",
    "\n",
    "\n",
    "This example tries to demonstrate a potential workflow from loading and processing (cloud removal, median-mosaicing, index calculation) satellite raster-data togehter with additional on-demand vector-data nd export them as cloud-optimized-geotiff for individual processing, e.g. in a local GIS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install packages\n",
    "!pip install --force-reinstall --extra-index-url=\"https://packages.dea.ga.gov.au\" hdstats==0.1.8.post1\n",
    "!pip install odc.io odc-apps-dc-tools pystac-client folium matplotlib mapclassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import scipy\n",
    "import matplotlib.colors as mcolours\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import dask.distributed\n",
    "import dask.utils\n",
    "import numpy as np\n",
    "import planetary_computer as pc\n",
    "import xarray as xr\n",
    "from IPython.display import display\n",
    "from pystac_client import Client\n",
    "from numpy.typing import DTypeLike\n",
    "from odc.stac import configure_rio, stac_load\n",
    "from datacube.utils.cog import write_cog\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dea_tools.classification import sklearn_flatten, sklearn_unflatten, KMeans_tree, fit_xr\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "import pystac_client\n",
    "import planetary_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading your area of interest as sahepfile (here: different conservation zones in Rhoen Highlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhoen = gpd.read_file('bwi2012.geojson')\n",
    "TOI = rhoen.loc[rhoen['Tnr'] == 17020] # tract of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentinel-2 data from the datacube\n",
    "\n",
    "Loading `Sentinel-2` satellite images through the datacube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to catalog variable\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "# from rhoen extent\n",
    "x_min=9.4\n",
    "x_max=10.5\n",
    "y_min=49.7\n",
    "y_max=51.2\n",
    "\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [x_max, y_min],\n",
    "            [x_min, y_min],\n",
    "            [x_min, y_max],\n",
    "            [x_max, y_max],\n",
    "            [x_max, y_min],\n",
    "        ]\n",
    "    ],}\n",
    "\n",
    "time_extents = ('2015-01-01', '2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    intersects=area_of_interest,\n",
    "    datetime=time_extents,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 70}} # cloud cover limit\n",
    ")\n",
    "\n",
    "# Check how many items were returned\n",
    "items = search.item_collection()\n",
    "print(f\"Returned {len(items)} Items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write ccordinates from toi\n",
    "minx = TOI.bounds.iloc[0,0]\n",
    "maxx = TOI.bounds.iloc[0,2]\n",
    "miny = TOI.bounds.iloc[0,1]\n",
    "maxy = TOI.bounds.iloc[0,3]\n",
    "\n",
    "x_min = minx\n",
    "x_max = maxx\n",
    "y_min = miny\n",
    "y_max = maxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lazy load data using dask-chunks\n",
    "dataset = stac_load(items,\n",
    "                    crs='EPSG:25832',\n",
    "                    y= [y_min, y_max],\n",
    "                    x= [x_min, x_max],\n",
    "                    resolution=10,\n",
    "                    chunks={},\n",
    "                    groupby=\"solar_day\",\n",
    "                    patch_url=pc.sign)\n",
    "# Take a look at the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename bands and delete unused\n",
    "dataset['coastal_aerosol'] = dataset.B01\n",
    "dataset['blue'] = dataset.B02\n",
    "dataset['green'] = dataset.B03\n",
    "dataset['red'] = dataset.B04\n",
    "dataset['red_edge1'] = dataset.B05\n",
    "dataset['red_edge2'] = dataset.B06\n",
    "dataset['red_edge3'] = dataset.B07\n",
    "dataset['nir'] = dataset.B08\n",
    "dataset['narrow_nir'] = dataset.B8A\n",
    "dataset['water_vapour'] = dataset.B09\n",
    "dataset['swir1'] = dataset.B11\n",
    "dataset['swir2'] = dataset.B12\n",
    "\n",
    "dataset = dataset.drop(['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lazy loaded data\n",
    "dataset = dataset.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a Sentinel-2 scene classification (scl-band) for masking clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scl_dict = {'0': 'no_data','1': 'defect','2': 'dark','3': 'cloud_shadow','4': 'vegetation','5': 'no_vegetation',\n",
    "#'6': 'water','7': 'no_class','8': 'cloud_med_prob','9': 'cloud_high_prob','10': 'cirrus','11': 'snow'}\n",
    "mask = dataset.SCL.isin([4,5,6,7,11])\n",
    "dataset['mask'] = mask\n",
    "dataset = dataset.where(dataset.SCL.isin([4, 5, 6, 7,11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cloudy/faulty pixel share which is accepted for using\n",
    "cloud_percent = .1\n",
    "\n",
    "clean_dataset=dataset.sel(time=pd.to_datetime(((dataset.sum(dim=['x','y'])/  ((dataset.dims)['y']*(dataset.dims)['x'])).time.where(\n",
    "    ((dataset.sum(dim=['x','y'])/((dataset.dims)['y']*(dataset.dims)['x'])).mask) > cloud_percent,drop=True)).values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_dataset.time.count(), dataset.time.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate indices for standard dataset\n",
    "NDVI = (dataset.nir - dataset.red)/(dataset.nir + dataset.red)\n",
    "NDWI = (dataset.nir - dataset.swir2)/(dataset.nir + dataset.swir2)\n",
    "NDRE1 = (dataset.red_edge2 - dataset.red_edge1)/(dataset.red_edge1 + dataset.red_edge1)\n",
    "NDRE2 = (dataset.narrow_nir - dataset.red_edge1)/(dataset.narrow_nir + dataset.red_edge1)\n",
    "\n",
    "dataset['ndvi'] = NDVI\n",
    "dataset['ndwi'] = NDWI\n",
    "dataset['ndre1'] = NDRE1\n",
    "dataset['ndre2'] = NDRE2\n",
    "\n",
    "# calculate indices for clean dataset\n",
    "NDVI = (clean_dataset.nir - clean_dataset.red)/(clean_dataset.nir + clean_dataset.red)\n",
    "NDWI = (clean_dataset.nir - clean_dataset.swir2)/(clean_dataset.nir + clean_dataset.swir2)\n",
    "NDRE1 = (clean_dataset.red_edge2 - clean_dataset.red_edge1)/(clean_dataset.red_edge1 + clean_dataset.red_edge1)\n",
    "NDRE2 = (clean_dataset.narrow_nir - clean_dataset.red_edge1)/(clean_dataset.narrow_nir + clean_dataset.red_edge1)\n",
    "\n",
    "clean_dataset['ndvi'] = NDVI\n",
    "clean_dataset['ndwi'] = NDWI\n",
    "clean_dataset['ndre1'] = NDRE1\n",
    "clean_dataset['ndre2'] = NDRE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cloud share\n",
    "It can be useful to know when we have clear looks on the earth surface. Therfore we examine the cloud coverage in total an during the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the prepared dataset using s2_mask() above\n",
    "times = list(clean_dataset.time.values)\n",
    "percentage_list = list((((clean_dataset.sum(dim=['x','y'])/\n",
    "                          ((clean_dataset.dims)['y']*(clean_dataset.dims)['x']))\n",
    "                        .mask).values)*100)\n",
    "data = {\"Acquisition time\": times,\n",
    "            \"Cloud free pixel[%]\": percentage_list}\n",
    "s2_cct = pd.DataFrame(data=data, columns = [\"Acquisition time\", \"Cloud free pixel[%]\"])\n",
    "s2_cct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cct = s2_cct\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "#plt.scatter(cloud_table[\"times\"].values, cloud_table[\"clean_percentage\"].values)\n",
    "plt.hist(cct[\"Cloud free pixel[%]\"], 18, facecolor='b', alpha=.5)\n",
    "plt.title('Cloud share', size='25')\n",
    "plt.xlabel('\\n clouds [% / overpass]', size=25)\n",
    "plt.ylabel('Overpasses [n]\\n', size=25)\n",
    "plt.yticks(list(range(0,200,20)),size='25')\n",
    "plt.xticks(list(range(0,100,10)),size='25')\n",
    "plt.axis([0,100, 0, 200])\n",
    "plt.grid(True)\n",
    "#plt.savefig('hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cct[\"Acquisition time\"]\n",
    "pix = cct[\"Cloud free pixel[%]\"].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "ax.plot(t,pix,'b-o', markersize=5)\n",
    "\n",
    "ax.xaxis.set_major_locator(mpl.dates.YearLocator())\n",
    "ax.xaxis.set_minor_locator(mpl.dates.MonthLocator((7)))\n",
    "\n",
    "ax.xaxis.set_major_formatter(mpl.dates.DateFormatter(\"\\n%Y\"))\n",
    "ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter(\"%b\"))\n",
    "\n",
    "ax.tick_params(axis='x',which='major', length=15, width=2, labelsize=25)\n",
    "ax.tick_params(axis='x',which='minor', length=5, width=2, labelsize=25)\n",
    "ax.tick_params(axis='y',which='major', length=5, width=2, labelsize=25)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel('cloud share [% / overpass]\\n', size=25)\n",
    "\n",
    "plt.savefig('cl_op.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vegetation analysis\n",
    "Here different types od vegetation anlysis is visualized, starting with the information on plants health ba the NDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veg_proxy = 'ndvi'# ndwi, ndre1, ndre2, ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset[veg_proxy].mean(['x', 'y']).plot.line('g-o', figsize=(11,4))\n",
    "plt.title('Zonal mean of vegetation timeseries');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a different resampling period\n",
    "resample_period='2W'\n",
    "# Rolling window size for smoothing the curve\n",
    "window=4\n",
    "# Perform bi-weekly median resampling mean smoothing\n",
    "veg_smooth=clean_dataset[veg_proxy].resample(time=resample_period).median().rolling(time=window, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate zonal mean and plot results\n",
    "veg_smooth_1D = veg_smooth.mean(['x', 'y'])\n",
    "veg_smooth_1D.plot.line('g-o', figsize=(15,5))\n",
    "_max=veg_smooth_1D.max()\n",
    "_min=veg_smooth_1D.min()\n",
    "\n",
    "plt.vlines(np.datetime64('2016-01-01'), ymin=_min, ymax=_max)\n",
    "plt.vlines(np.datetime64('2017-01-01'), ymin=_min, ymax=_max)\n",
    "plt.vlines(np.datetime64('2018-01-01'), ymin=_min, ymax=_max)\n",
    "plt.vlines(np.datetime64('2019-01-01'), ymin=_min, ymax=_max)\n",
    "plt.vlines(np.datetime64('2020-01-01'), ymin=_min, ymax=_max)\n",
    "plt.vlines(np.datetime64('2021-01-01'), ymin=_min, ymax=_max)\n",
    "plt.vlines(np.datetime64('2022-01-01'), ymin=_min, ymax=_max)\n",
    "plt.title(veg_proxy+' time-series, year start/ends marked with vertical lines')\n",
    "#plt.savefig('ndvi_all') #saving to file\n",
    "plt.ylabel(veg_proxy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target aggregation type of the curve fit. \n",
    "# Input can be either 'mean' or 'median'.\n",
    "curve_fit_target = 'median'\n",
    "\n",
    "# The maximum number of data points that appear along time in each plot.\n",
    "# If more than this number of data points need to be plotted, a grid of plots will be created.\n",
    "max_times_per_plot = 50\n",
    "\n",
    "# Select the binning approach for the vegetation index. Choose one from the list below. \n",
    "#  None         = do not bin the data\n",
    "# 'week'        = bin the data by week with an extended time axis\n",
    "# 'month'       = bin the data by month with an extended time axis\n",
    "# 'weekofyear'  = bin the data by week and years using a single year time axis\n",
    "# 'monthofyear' = bin the data by month and years using a single year time axis\n",
    "bin_by = 'month'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed function are under the three dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def xarray_time_series_plot(dataset, plot_descs, x_coord='x',\n",
    "                            y_coord='y', fig_params=None,\n",
    "                            fig=None, ax=None, show_legend=True, title=None,\n",
    "                            max_times_per_plot=None, max_cols=1):\n",
    "    \"\"\"\n",
    "    Plot data variables in an `xarray.Dataset` with different\n",
    "    plot types and optional curve fits. Handles data binned with\n",
    "    `xarray.Dataset` methods `resample()` and `groupby()`. That is, it handles data\n",
    "    binned along time (e.g. by week) or across years (e.g. by week of year).\n",
    "    Parameters\n",
    "    -----------\n",
    "    dataset: xarray.Dataset\n",
    "        A Dataset containing some bands like NDVI or WOFS.\n",
    "        It must have time, x, and y coordinates with names specified by\n",
    "        the 'x_coord' and 'y_coord' parameters.\n",
    "    plot_descs: dict\n",
    "        Dictionary mapping names of DataArrays in the Dataset to plot to\n",
    "        dictionaries mapping aggregation types (e.g. 'mean', 'median') to\n",
    "        lists of dictionaries mapping plot types\n",
    "        (e.g. 'line', 'box', 'scatter') to keyword arguments for plotting.\n",
    "        Aggregation happens within time slices and can be many-to-many or many-to-one.\n",
    "        Some plot types require many-to-many aggregation (e.g. 'none'), and some other plot types\n",
    "        require many-to-one aggregation (e.g. 'mean'). Aggregation types can be any of\n",
    "        ['min', 'mean', 'median', 'none', 'max'], with 'none' performing no aggregation.\n",
    "        Plot types can be any of\n",
    "        ['scatter', 'line', 'box', 'gaussian', 'gaussian_filter', 'poly', 'cubic_spline', 'fourier'].\n",
    "        Here are the required arguments, with format {plot_type: {arg_name: (data_type[, description]}}:\n",
    "        {'poly': {'degree': (int, \"the degree of the polynomial to fit.\")}}\n",
    "        Here are the optional arguments, with format {plot_type: {arg_name: (data_type[, description]}}:\n",
    "        # See matplotlib.axes.Axes.boxplot() for more information.\n",
    "        {'box': {'boxprops': dict, 'flierprops': dict, 'showfliers': bool},\n",
    "        # See gaussian_filter_fit() in data_cube_utilities/curve_fitting.py for more information.\n",
    "        'gaussian_filter': {'sigma': numeric},\n",
    "        'fourier':\n",
    "        {'extrap_time': (string, \"a positive integer followed by Y, M, or D -\n",
    "        year, month, or day - specifying the amount of time to extrapolate over.\"),\n",
    "        'extrap_color': (matplotlib color, \"a matplotlib color to color the extrapolated data with.\")\n",
    "        }}\n",
    "        Additionally, all of the curve fits (['gaussian', 'gaussian_filter', 'poly',\n",
    "        'cubic_spline', 'fourier']) support an optional 'smooth' boolean parameter.\n",
    "        If true, the curve fit is smoothed, otherwise it will look no smoother than the original data.\n",
    "        Here is an example:\n",
    "        {'ndvi':\n",
    "        {'mean':[{'line':{'color':'forestgreen', 'alpha':alpha}}],\n",
    "        'none':[{'box':{'boxprops':{'facecolor':'forestgreen','alpha':alpha},'showfliers':False}}]}}\n",
    "        This example will create a green line plot of the mean of the 'ndvi' band\n",
    "        as well as a green box plot of the 'ndvi' band.\n",
    "    x_coord, y_coord: str\n",
    "        Names of the x and y coordinates in `dataset`.\n",
    "    fig_params: dict\n",
    "        Figure parameters dictionary (e.g. {'figsize':(12,6)}). Used to create a Figure\n",
    "        `if fig is None and ax is None`.\n",
    "    fig: matplotlib.figure.Figure\n",
    "        The figure to use for the plot.\n",
    "        If only `fig` is supplied, the Axes object used will be the first. This\n",
    "        argument is ignored if ``max_times_per_plot`` is less than the number of times.\n",
    "    ax: matplotlib.axes.Axes\n",
    "        The axes to use for the plot. This argument is ignored if\n",
    "        ``max_times_per_plot`` is less than the number of times.\n",
    "    show_legend: bool\n",
    "        Whether or not to show the legend.\n",
    "    title: str\n",
    "        The title of each subplot. Note that a date range enclosed in parenthesis\n",
    "        will be postpended whether this is specified or not.\n",
    "    max_times_per_plot: int\n",
    "        The maximum number of times per plot. If specified, multiple plots may be created,\n",
    "        with each plot having as close to `num_times/max_times_per_plot` number of points\n",
    "        as possible, where `num_times` is the total number of plotting points, including\n",
    "        extrapolations. The plots will be arranged in a row-major grid, with the number\n",
    "        of columns being at most `max_cols`.\n",
    "    max_cols: int\n",
    "        The maximum number of columns in the plot grid.\n",
    "    Returns\n",
    "    -------\n",
    "    fig: matplotlib.figure.Figure\n",
    "        The figure containing the plot grid.\n",
    "    plotting_data: dict\n",
    "        A dictionary mapping 3-tuples of data array names, aggregation types, and plot types\n",
    "        (e.g. ('ndvi', 'none', 'box')) to `xarray.DataArray` objects of the data that was\n",
    "        plotted for those combinations of aggregation types and plot types.\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError:\n",
    "        If an aggregation type is not possible for a plot type\n",
    "    \"\"\"\n",
    "    fig_params = {} if fig_params is None else fig_params\n",
    "\n",
    "    # Lists of plot types that can and cannot accept many-to-one aggregation\n",
    "    # for each time slice, as well as plot types that support extrapolation.\n",
    "    plot_types_requiring_aggregation = ['line', 'gaussian', 'gaussian_filter', 'poly',\n",
    "                                        'cubic_spline', 'fourier']\n",
    "    plot_types_handling_aggregation = ['scatter'] + plot_types_requiring_aggregation\n",
    "    plot_types_not_handling_aggregation = ['box']\n",
    "    plot_types_curve_fit = ['gaussian', 'gaussian_filter', 'poly',\n",
    "                            'cubic_spline', 'fourier']\n",
    "    plot_types_supporting_extrapolation = ['fourier']\n",
    "    all_plot_types = list(set(plot_types_requiring_aggregation + plot_types_handling_aggregation + \\\n",
    "                              plot_types_not_handling_aggregation + plot_types_curve_fit + \\\n",
    "                              plot_types_supporting_extrapolation))\n",
    "\n",
    "    # Aggregation types that aggregate all values for a given time to one value.\n",
    "    many_to_one_agg_types = ['min', 'mean', 'median', 'max']\n",
    "    # Aggregation types that aggregate to many values or do not aggregate.\n",
    "    many_to_many_agg_types = ['none']\n",
    "    all_agg_types = many_to_one_agg_types + many_to_many_agg_types\n",
    "\n",
    "    # Determine how the data was aggregated, if at all.\n",
    "    possible_time_agg_strs = ['time', 'week', 'month']\n",
    "    time_agg_str = 'time'\n",
    "    for possible_time_agg_str in possible_time_agg_strs:\n",
    "        if possible_time_agg_str in list(dataset.coords):\n",
    "            time_agg_str = possible_time_agg_str\n",
    "            break\n",
    "    # Make the data 2D - time and a stack of all other dimensions.\n",
    "    all_plotting_data_arrs = list(plot_descs.keys())\n",
    "    all_plotting_data = dataset[all_plotting_data_arrs]\n",
    "    all_times = all_plotting_data[time_agg_str].values\n",
    "    # Mask out times for which no data variable to plot has any non-NaN data.\n",
    "    nan_mask_data_vars = list(all_plotting_data[all_plotting_data_arrs] \\\n",
    "                              .notnull().data_vars.values())\n",
    "    for i, data_var in enumerate(nan_mask_data_vars):\n",
    "        time_nan_mask = data_var if i == 0 else time_nan_mask | data_var\n",
    "    time_nan_mask = time_nan_mask.any([x_coord, y_coord])\n",
    "    times_not_all_nan = all_times[time_nan_mask.values]\n",
    "    non_nan_plotting_data = all_plotting_data.loc[{time_agg_str: times_not_all_nan}]\n",
    "\n",
    "    # Determine the number of extrapolation data points. #\n",
    "    extrap_day_range = 0\n",
    "    n_extrap_pts = 0\n",
    "    # For each data array to plot...\n",
    "    for data_arr_name, agg_dict in plot_descs.items():\n",
    "        # For each aggregation type (e.g. 'mean', 'median')...\n",
    "        for agg_type, plot_dicts in agg_dict.items():\n",
    "            # For each plot for this aggregation type...\n",
    "            for i, plot_dict in enumerate(plot_dicts):\n",
    "                for plot_type, plot_kwargs in plot_dict.items():\n",
    "                    # Only check the plot types supporting extrapolation.\n",
    "                    if plot_type == 'fourier':\n",
    "                        curr_extrap_day_range = 0\n",
    "                        n_predict = 0  # Default to no extrapolation.\n",
    "                        # Addressing this way to modify `plot_descs`.\n",
    "                        extrap_time = plot_kwargs.get('extrap_time', None)\n",
    "                        if extrap_time is not None:\n",
    "                            assert time_agg_str == 'time', \\\n",
    "                                \"Extrapolating for data with a time dimension other than 'time' - \" \\\n",
    "                                \"such as 'month', or 'week' - is not supported. A time dimension of 'month' \" \\\n",
    "                                \"or 'week' denotes data aggregated for each month or week across years, so \" \\\n",
    "                                \"extrapolation is meaningless in that case. Support for a time dimension of 'year' \" \\\n",
    "                                \"has not yet been added.\"\n",
    "                            # Determine the number of points to extrapolate (in an approximate manner).\n",
    "                            # First find the time range of the given data.\n",
    "                            first_last_days = list(map(lambda np_dt_64: _n64_to_datetime(np_dt_64),\n",
    "                                                       non_nan_plotting_data.time.values[[0, -1]]))\n",
    "                            year_range = first_last_days[1].year - first_last_days[0].year\n",
    "                            month_range = first_last_days[1].month - first_last_days[0].month\n",
    "                            day_range = first_last_days[1].day - first_last_days[0].day\n",
    "                            day_range = year_range * 365.25 + month_range * 30 + day_range\n",
    "                            # Then find the time range of the extrapolation string.\n",
    "                            fields = re.match(r\"(?P<num>[0-9]{0,5})(?P<unit>[YMD])\", extrap_time)\n",
    "                            assert fields is not None, \\\n",
    "                                r\"For the '{}' DataArray: When using 'fourier' as \" \\\n",
    "                                \"the fit type, if the 'extrap_time' parameter is supplied, it must be \" \\\n",
    "                                \"a string containing a positive integer followed by one of ['Y', 'M', or 'D'].\" \\\n",
    "                                    .format(data_arr_name)\n",
    "                            num, unit = int(fields['num']), fields['unit']\n",
    "                            days_per_unit = dict(Y=365.25, M=30, D=1)[unit]\n",
    "                            curr_extrap_day_range = num * days_per_unit\n",
    "                            n_predict = round(len(non_nan_plotting_data[time_agg_str]) *\n",
    "                                              (curr_extrap_day_range / day_range))\n",
    "                            plot_descs[data_arr_name][agg_type][i][plot_type] \\\n",
    "                                ['n_predict'] = n_predict\n",
    "                        # This parameter is used by get_curvefit() later.\n",
    "                        extrap_day_range = max(extrap_day_range, curr_extrap_day_range)\n",
    "                        n_extrap_pts = max(n_extrap_pts, n_predict)\n",
    "\n",
    "    # Collect (1) the times not containing only NaN values and (2) the extrapolation times.\n",
    "    if time_agg_str == 'time' and len(times_not_all_nan) > 0:\n",
    "        first_extrap_time = times_not_all_nan[-1] + np.timedelta64(extrap_day_range, 'D') / n_extrap_pts\n",
    "        last_extrap_time = times_not_all_nan[-1] + np.timedelta64(extrap_day_range, 'D')\n",
    "        extrap_times = np.linspace(_n64_datetime_to_scalar(first_extrap_time),\n",
    "                                   _n64_datetime_to_scalar(last_extrap_time), num=n_extrap_pts)\n",
    "        extrap_times = np.array(list(map(_scalar_to_n64_datetime, extrap_times)))\n",
    "        times_not_all_nan_and_extrap = np.concatenate((times_not_all_nan, extrap_times)) \\\n",
    "            if len(extrap_times) > 0 else times_not_all_nan\n",
    "    else:\n",
    "        times_not_all_nan_and_extrap = times_not_all_nan\n",
    "    # Compute all of the plotting data - handling aggregations and extrapolations.\n",
    "    plotting_data_not_nan_and_extrap = {}  # Maps data arary names to plotting data (NumPy arrays).\n",
    "    # Get the x locations of data points not filled with NaNs and the x locations of extrapolation points.\n",
    "    epochs = np.array(list(map(n64_to_epoch, times_not_all_nan_and_extrap))) \\\n",
    "        if time_agg_str == 'time' else times_not_all_nan_and_extrap\n",
    "    epochs_not_extrap = epochs[:len(times_not_all_nan)]\n",
    "\n",
    "    # Handle aggregations and curve fits. #\n",
    "    # For each data array to plot...\n",
    "    for data_arr_name, agg_dict in plot_descs.items():\n",
    "        data_arr_plotting_data = non_nan_plotting_data[data_arr_name]\n",
    "        # For each aggregation type (e.g. 'mean', 'median')...\n",
    "        for agg_type, plot_dicts in agg_dict.items():\n",
    "            # For each plot for this aggregation type...\n",
    "            for i, plot_dict in enumerate(plot_dicts):\n",
    "                for plot_type, plot_kwargs in plot_dict.items():\n",
    "                    assert plot_type in all_plot_types, \\\n",
    "                        r\"For the '{}' DataArray: plot_type '{}' not recognized\" \\\n",
    "                            .format(data_arr_name, plot_type)\n",
    "\n",
    "                    # Ensure aggregation types are legal for this data.\n",
    "                    # Some plot types require aggregation.\n",
    "                    if plot_type in plot_types_requiring_aggregation:\n",
    "                        if agg_type not in many_to_one_agg_types:\n",
    "                            raise ValueError(\"For the '{}' DataArray: the plot type \"\n",
    "                                             \"'{}' only accepts many-to-one aggregation (currently using '{}'). \"\n",
    "                                             \"Please pass any of {} as the aggregation type \"\n",
    "                                             \"or change the plot type.\".format(data_arr_name, \\\n",
    "                                                                               plot_type, agg_type,\n",
    "                                                                               many_to_one_agg_types))\n",
    "                    # Some plot types cannot accept many-to-one aggregation.\n",
    "                    if plot_type not in plot_types_handling_aggregation:\n",
    "                        if agg_type not in many_to_many_agg_types:\n",
    "                            raise ValueError(\"For the '{}' DataArray: \"\n",
    "                                             \"the plot type '{}' only accepts many-to-many aggregation \"\n",
    "                                             \"(currently using '{}'). Please pass any of {} as \"\n",
    "                                             \"the aggregation type or change the plot type.\"\n",
    "                                             .format(data_arr_name, plot_type, agg_type,\n",
    "                                                     many_to_many_agg_types))\n",
    "\n",
    "                    # Aggregate if necessary.\n",
    "                    y = data_arr_plotting_data\n",
    "                    if agg_type == 'min':\n",
    "                        y = y.min([x_coord, y_coord])\n",
    "                    if agg_type == 'mean':\n",
    "                        y = y.mean([x_coord, y_coord])\n",
    "                    if agg_type == 'median':\n",
    "                        y = y.median([x_coord, y_coord])\n",
    "                    if agg_type == 'max':\n",
    "                        y = y.max([x_coord, y_coord])\n",
    "\n",
    "                    # Handle curve fits.\n",
    "                    if plot_type in plot_types_curve_fit:\n",
    "                        smooth = plot_kwargs.get('smooth', True)\n",
    "                        # Create the curve fit.\n",
    "                        x_smooth = None if smooth else epochs_not_extrap\n",
    "                        data_arr_epochs, y = get_curvefit(epochs_not_extrap, y.values, fit_type=plot_type,\n",
    "                                                          x_smooth=x_smooth, fit_kwargs=plot_kwargs)\n",
    "                        # Convert time stamps to NumPy datetime objects.\n",
    "                        data_arr_times = np.array(list(map(_scalar_to_n64_datetime, data_arr_epochs))) \\\n",
    "                            if time_agg_str == 'time' else data_arr_epochs\n",
    "                        # Convert the NumPy array into an xarray DataArray.\n",
    "                        coords = {time_agg_str: data_arr_times}\n",
    "                        dims = list(coords.keys())\n",
    "                        y = xr.DataArray(y, coords=coords, dims=dims)\n",
    "                    plotting_data_not_nan_and_extrap[(data_arr_name, agg_type, plot_type)] = y\n",
    "\n",
    "    # Handle the potential for multiple plots.\n",
    "    max_times_per_plot = len(times_not_all_nan_and_extrap) if max_times_per_plot is None else \\\n",
    "        max_times_per_plot\n",
    "    num_times = len(times_not_all_nan_and_extrap)\n",
    "    num_plots = int(np.ceil(num_times / max_times_per_plot))\n",
    "    num_times_per_plot = round(num_times / num_plots) if num_plots != 0 else 0\n",
    "    num_cols = min(num_plots, max_cols)\n",
    "    num_rows = int(np.ceil(num_plots / num_cols)) if num_cols != 0 else 0\n",
    "    # Set a reasonable figsize if one is not set in `fig_params`.\n",
    "    fig_params.setdefault('figsize', (12 * num_cols, 6 * num_rows))\n",
    "    fig = plt.figure(**fig_params) if fig is None else fig\n",
    "\n",
    "    # Check if there are no plots to make.\n",
    "    if num_plots == 0:\n",
    "        return fig, plotting_data_not_nan_and_extrap\n",
    "\n",
    "    # Create each plot. #\n",
    "    for time_ind, ax_ind in zip(range(0, len(times_not_all_nan_and_extrap), num_times_per_plot),\n",
    "                                range(num_plots)):\n",
    "        # The time bounds of this canvas (or \"Axes object\" or \"plot grid cell\").\n",
    "        ax_lower_time_bound_ind, ax_upper_time_bound_ind = \\\n",
    "            time_ind, min(time_ind + num_times_per_plot, len(times_not_all_nan_and_extrap))\n",
    "        # Retrieve or create the axes if necessary.\n",
    "        if len(times_not_all_nan_and_extrap) <= num_times_per_plot:\n",
    "            fig, ax = retrieve_or_create_fig_ax(fig, ax, **fig_params)\n",
    "        else:\n",
    "            ax = fig.add_subplot(num_rows, num_cols, ax_ind + 1)\n",
    "        ax_times_not_all_nan_and_extrap = \\\n",
    "            times_not_all_nan_and_extrap[ax_lower_time_bound_ind:ax_upper_time_bound_ind]\n",
    "        ax_time_bounds = ax_times_not_all_nan_and_extrap[[0, -1]]\n",
    "        ax_epochs = epochs[ax_lower_time_bound_ind:ax_upper_time_bound_ind]\n",
    "        ax_x_locs = np_scale(ax_epochs if time_agg_str == 'time' else ax_times_not_all_nan_and_extrap)\n",
    "\n",
    "        # Data variable plots within each plot.\n",
    "        data_arr_plots = []\n",
    "        legend_labels = []\n",
    "        # For each data array to plot...\n",
    "        for data_arr_name, agg_dict in plot_descs.items():\n",
    "            # For each aggregation type (e.g. 'mean', 'median')...\n",
    "            for agg_type, plot_dicts in agg_dict.items():\n",
    "                # For each plot for this aggregation type...\n",
    "                for plot_dict in plot_dicts:\n",
    "                    for plot_type, plot_kwargs in plot_dict.items():\n",
    "                        # Determine the legend label for this plot.\n",
    "                        plot_type_str = \\\n",
    "                            {'scatter': 'scatterplot', 'line': 'lineplot',\n",
    "                             'box': 'boxplot', 'gaussian': 'gaussian fit',\n",
    "                             'gaussian_filter': 'gaussian filter fit',\n",
    "                             'poly': 'degree {} polynomial fit',\n",
    "                             'cubic_spline': 'cubic spline fit',\n",
    "                             'fourier': 'Fourier fit ({} harmonics)'}[plot_type]\n",
    "                        if plot_type == 'poly':\n",
    "                            assert 'degree' in plot_kwargs, \\\n",
    "                                r\"For the '{}' DataArray: When using 'poly' as \" \\\n",
    "                                \"the fit type, the fit kwargs must have 'degree' \" \\\n",
    "                                \"specified.\".format(data_arr_name)\n",
    "                            plot_type_str = plot_type_str.format(\n",
    "                                plot_kwargs.get('degree'))\n",
    "                        if plot_type == 'fourier':\n",
    "                            plot_type_str = plot_type_str.format(\n",
    "                                plot_kwargs.get('n_harm', default_fourier_n_harm))\n",
    "                        # Legend labels for the non-extrapolation\n",
    "                        # and extrapolation segments\n",
    "                        plot_type_strs = []\n",
    "\n",
    "                        # Remove plot kwargs that are not recognized\n",
    "                        # by plotting methods (cause errors).\n",
    "                        plot_kwargs = plot_kwargs.copy()\n",
    "                        plot_kwargs.pop('extrap_time', None)\n",
    "                        plot_kwargs.pop('n_predict', None)\n",
    "                        plot_kwargs.pop('smooth', None)\n",
    "                        plot_kwargs.pop('degree', None)  # 'degree'\n",
    "                        plot_kwargs.pop('n_harm', None)  # 'fourier'\n",
    "\n",
    "                        # Handle default plot kwargs.\n",
    "                        if plot_type == 'box':\n",
    "                            plot_kwargs.setdefault('boxprops',\n",
    "                                                   dict(facecolor='orange'))\n",
    "                            plot_kwargs.setdefault('flierprops',\n",
    "                                                   dict(marker='o', markersize=0.5))\n",
    "                            plot_kwargs.setdefault('showfliers', False)\n",
    "\n",
    "                        # Retrieve the plotting data.\n",
    "                        y = plotting_data_not_nan_and_extrap[\n",
    "                            (data_arr_name, agg_type, plot_type)]\n",
    "                        y = y.sel({time_agg_str:\n",
    "                                       slice(ax_time_bounds[0], ax_time_bounds[1])})\n",
    "\n",
    "                        # Handle cases of insufficient data for this section of the plot.\n",
    "                        not_nat_times = None\n",
    "                        if time_agg_str == 'time':\n",
    "                            not_nat_times = ~np.isnat(y[time_agg_str].values)\n",
    "                        else:\n",
    "                            not_nat_times = ~np.isnan(y[time_agg_str].values)\n",
    "                        num_unique_times_y = len(np.unique(y[time_agg_str].values[not_nat_times]))\n",
    "                        if num_unique_times_y == 0:  # There is no data.\n",
    "                            continue\n",
    "                        if num_unique_times_y == 1:  # There is 1 data point.\n",
    "                            plot_type = 'scatter'\n",
    "                            plot_kwargs = {}\n",
    "\n",
    "                        data_arr_epochs = \\\n",
    "                            np.array(list(map(n64_to_epoch, y[time_agg_str].values))) \\\n",
    "                                if time_agg_str == 'time' else \\\n",
    "                                ax_times_not_all_nan_and_extrap\n",
    "                        data_arr_x_locs = np.interp(data_arr_epochs,\n",
    "                                                    ax_epochs, ax_x_locs)\n",
    "                        data_arr_time_bounds = y[time_agg_str].values[[0, -1]]\n",
    "\n",
    "                        # Determine if this plotting data includes extrapolated values.\n",
    "                        data_arr_non_extrap_time_bounds = None\n",
    "                        data_arr_has_non_extrap = \\\n",
    "                            data_arr_time_bounds[0] < times_not_all_nan[-1]\n",
    "                        if data_arr_has_non_extrap:\n",
    "                            data_arr_non_extrap_time_bounds = \\\n",
    "                                [data_arr_time_bounds[0], min(data_arr_time_bounds[1],\n",
    "                                                              times_not_all_nan[-1])]\n",
    "                            # Because the data could be smoothed, the last\n",
    "                            # non-extrapolation time is the last time before\n",
    "                            # or at the last non-extrapolation time\n",
    "                            # for the original data.\n",
    "                            non_extrap_plot_last_time = data_arr_non_extrap_time_bounds[1]\n",
    "                            if num_unique_times_y > 1:\n",
    "                                non_extrap_plot_last_time = \\\n",
    "                                    y.sel({time_agg_str: data_arr_non_extrap_time_bounds[1]},\n",
    "                                          method='ffill')[time_agg_str].values\n",
    "                            data_arr_non_extrap_plotting_time_bounds = [data_arr_non_extrap_time_bounds[0],\n",
    "                                                                        non_extrap_plot_last_time]\n",
    "\n",
    "                        data_arr_extrap_time_bounds = None\n",
    "                        data_arr_has_extrap = times_not_all_nan[-1] < data_arr_time_bounds[1]\n",
    "                        if data_arr_has_extrap:\n",
    "                            data_arr_extrap_time_bounds = [max(data_arr_time_bounds[0],\n",
    "                                                               extrap_times[0]),\n",
    "                                                           data_arr_time_bounds[1]]\n",
    "                            # Because the data could be smoothed, the first extrapolation time\n",
    "                            # is the first time after the last non-extrapolation time for the original data.\n",
    "                            extrap_plot_first_time = \\\n",
    "                                y.sel({time_agg_str: data_arr_non_extrap_time_bounds[1]},\n",
    "                                      method='ffill')[time_agg_str].values \\\n",
    "                                    if data_arr_has_non_extrap else \\\n",
    "                                    data_arr_time_bounds[0]\n",
    "                            data_arr_extrap_plotting_time_bounds = [extrap_plot_first_time,\n",
    "                                                                    data_arr_extrap_time_bounds[1]]\n",
    "\n",
    "                        # Separate non-extrapolation and extrapolation data.\n",
    "                        if data_arr_has_non_extrap:\n",
    "                            data_arr_non_extrap = \\\n",
    "                                y.sel({time_agg_str: slice(*data_arr_non_extrap_plotting_time_bounds)})\n",
    "                            data_arr_non_extrap_epochs = \\\n",
    "                                np.array(list(map(n64_to_epoch, data_arr_non_extrap[time_agg_str].values))) \\\n",
    "                                    if time_agg_str == 'time' else data_arr_non_extrap[time_agg_str].values\n",
    "                            data_arr_non_extrap_x_locs = \\\n",
    "                                np.interp(data_arr_non_extrap_epochs, ax_epochs, ax_x_locs)\n",
    "                            # Format plotting kwargs for the non-extrapolation data.\n",
    "                            plot_kwargs_non_extrap = plot_kwargs.copy()\n",
    "                            plot_kwargs_non_extrap.pop('extrap_color', None)\n",
    "                        if data_arr_has_extrap:\n",
    "                            # Include the last non-extrapolation point so the\n",
    "                            # non-extrapolation and extrapolation lines connect.\n",
    "                            data_arr_extrap = \\\n",
    "                                y.sel({time_agg_str: slice(*data_arr_extrap_plotting_time_bounds)})\n",
    "                            data_arr_extrap_epochs = \\\n",
    "                                np.array(list(map(n64_to_epoch, data_arr_extrap[time_agg_str].values))) \\\n",
    "                                    if time_agg_str == 'time' else data_arr_extrap[time_agg_str].values\n",
    "                            data_arr_extrap_x_locs = \\\n",
    "                                np.interp(data_arr_extrap_epochs, ax_epochs, ax_x_locs)\n",
    "                            # Format plotting kwargs for the extrapolation data.\n",
    "                            plot_kwargs_extrap = plot_kwargs.copy()\n",
    "                            extrap_color = plot_kwargs_extrap.pop('extrap_color', None)\n",
    "                            if extrap_color is not None:\n",
    "                                plot_kwargs_extrap['color'] = extrap_color\n",
    "\n",
    "                        # Specify non-extrap and extrap plotting args.\n",
    "                        if data_arr_has_non_extrap:\n",
    "                            plot_args_non_extrap = \\\n",
    "                                [data_arr_non_extrap_x_locs, data_arr_non_extrap]\n",
    "                        if data_arr_has_extrap:\n",
    "                            plot_args_extrap = \\\n",
    "                                [data_arr_extrap_x_locs, data_arr_extrap]\n",
    "\n",
    "                        # Actually create the plot.\n",
    "                        def create_plot(x_locs, data_arr, **plot_kwargs):\n",
    "                            \"\"\"\n",
    "                            Creates a plot\n",
    "                            Parameters\n",
    "                            ----------\n",
    "                            x_locs: xarray.DataArray\n",
    "                                A 1D `xarray.DataArray` containing ascending values\n",
    "                                in range [0,1], denoting the x locations on the current\n",
    "                                canvas at which to plot data with corresponding time\n",
    "                                indicies in `data_arr`.\n",
    "                            data_arr: xarray.DataArray\n",
    "                                An `xarray.DataArray` containing a dimension named\n",
    "                                `time_agg_str` (the value of that variable in this context).\n",
    "                            Returns\n",
    "                            -------\n",
    "                            plot_obj: matplotlib.artist.Artist\n",
    "                                The plot.\n",
    "                            \"\"\"\n",
    "                            plot_obj = None\n",
    "                            if plot_type == 'scatter':\n",
    "                                data_arr_dims = list(data_arr.dims)\n",
    "                                data_arr_flat = data_arr.stack(flat=data_arr_dims)\n",
    "                                plot_obj = ax.scatter(x_locs, data_arr_flat)\n",
    "                            elif plot_type in ['line', 'gaussian', 'gaussian_filter',\n",
    "                                               'poly', 'cubic_spline', 'fourier']:\n",
    "                                plot_obj = ax.plot(x_locs, data_arr)[0]\n",
    "                            elif plot_type == 'box':\n",
    "                                boxplot_nan_mask = ~np.isnan(data_arr)\n",
    "                                # Data formatted for matplotlib.pyplot.boxplot().\n",
    "                                filtered_formatted_data = []\n",
    "                                for i, (d, m) in enumerate(zip(data_arr.values,\n",
    "                                                               boxplot_nan_mask.values)):\n",
    "                                    if len(d[m] != 0):\n",
    "                                        filtered_formatted_data.append(d[m])\n",
    "                                box_width = 0.5 * np.min(np.diff(x_locs)) \\\n",
    "                                    if len(x_locs) > 1 else 0.5\n",
    "                                # `manage_ticks=False` to avoid excessive padding on x-axis.\n",
    "                                bp = ax.boxplot(filtered_formatted_data,\n",
    "                                                widths=[box_width] * len(filtered_formatted_data),\n",
    "                                                positions=x_locs, patch_artist=True,\n",
    "                                                manage_ticks=False, **plot_kwargs)\n",
    "                                plot_obj = bp['boxes'][0]\n",
    "                            return plot_obj\n",
    "\n",
    "                        if data_arr_has_non_extrap:\n",
    "                            plot_obj = create_plot(*plot_args_non_extrap, **plot_kwargs_non_extrap)\n",
    "                            data_arr_plots.append(plot_obj)\n",
    "                            plot_type_strs.append(plot_type_str)\n",
    "                        if data_arr_has_extrap and plot_type in plot_types_supporting_extrapolation:\n",
    "                            plot_obj = create_plot(*plot_args_extrap, **plot_kwargs_extrap)\n",
    "                            data_arr_plots.append(plot_obj)\n",
    "                            plot_type_strs.append('extrapolation of ' + plot_type_str)\n",
    "                        plot_type_str_suffix = ' of {}'.format(agg_type) if agg_type != 'none' else ''\n",
    "                        plot_type_strs = [plot_type_str + plot_type_str_suffix\n",
    "                                          for plot_type_str in plot_type_strs]\n",
    "                        [legend_labels.append('{} of {}'.format(plot_type_str, data_arr_name))\n",
    "                         for plot_type_str in plot_type_strs]\n",
    "\n",
    "        # Label the axes and create the legend.\n",
    "        date_strs = \\\n",
    "            np.array(list(map(lambda time: np_dt64_to_str(time), ax_times_not_all_nan_and_extrap))) \\\n",
    "                if time_agg_str == 'time' else \\\n",
    "                naive_months_ticks_by_week(ax_times_not_all_nan_and_extrap) \\\n",
    "                    if time_agg_str in ['week', 'weekofyear'] else \\\n",
    "                    month_ints_to_month_names(ax_times_not_all_nan_and_extrap)\n",
    "        plt.xticks(ax_x_locs, date_strs, rotation=45, ha='right', rotation_mode='anchor')\n",
    "        if show_legend:\n",
    "            ax.legend(handles=data_arr_plots, labels=legend_labels, loc='best')\n",
    "        title_postpend = \" ({} to {})\".format(date_strs[0], date_strs[-1])\n",
    "        title_prepend = \"Figure {}\".format(ax_ind) if title is None else title\n",
    "        ax.set_title(title_prepend + title_postpend)\n",
    "    return fig, plotting_data_not_nan_and_extrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _scalar_to_n64_datetime(scalar):\n",
    "    \"\"\"\n",
    "    Converts a floating point number to a NumPy datetime64 object.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dt64: numpy.datetime64\n",
    "        The NumPy datetime64 object representing the datetime of the scalar argument.\n",
    "    \"\"\"\n",
    "    return (scalar * np.timedelta64(1, 's')) + np.datetime64('1970-01-01T00:00:00Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _n64_datetime_to_scalar(dt64):\n",
    "    \"\"\"\n",
    "    Converts a NumPy datetime64 object to the number of seconds since \n",
    "    midnight, January 1, 1970, as a NumPy float64.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    scalar: numpy.float64\n",
    "        The number of seconds since midnight, January 1, 1970, as a NumPy float64.\n",
    "    \"\"\"\n",
    "    return (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n64_to_epoch(timestamp):\n",
    "    ts = pd.to_datetime(str(timestamp))\n",
    "    time_format = \"%Y-%m-%d\"\n",
    "    ts = ts.strftime(time_format)\n",
    "    epoch = int(time.mktime(time.strptime(ts, time_format)))\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_curvefit(x, y, fit_type, x_smooth=None, n_pts=2000, fit_kwargs=None):\n",
    "    \"\"\"\n",
    "    Gets a curve fit given x values, y values, a type of curve, and parameters for that curve.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "        A 1D NumPy array. The x values to fit to.\n",
    "    y: np.ndarray\n",
    "        A 1D NumPy array. The y values to fit to.\n",
    "    fit_type: str\n",
    "        The type of curve to fit. One of \n",
    "        ['gaussian', 'gaussian_filter', 'poly', \n",
    "        'cubic_spline', 'fourier'].\n",
    "        The option 'gaussian' creates a Gaussian fit.\n",
    "        The option 'gaussian_filter' creates a Gaussian filter fit.\n",
    "        The option 'poly' creates a polynomial fit.\n",
    "        The option 'cubic_spline' creates a cubic spline fit.\n",
    "        The option 'fourier' creates a Fourier curve fit.\n",
    "    x_smooth: list-like\n",
    "        The exact x values to interpolate for. Supercedes `n_pts`.\n",
    "    n_pts: int\n",
    "        The number of evenly spaced points spanning the range of `x` to interpolate for.\n",
    "    fit_kwargs: dict\n",
    "        Keyword arguments for the selected fit type.\n",
    "        In the case of `fit_type == 'poly'`, this must contain a 'degree' entry (an int),\n",
    "        which is the degree of the polynomial to fit.\n",
    "        In the case of `fit_type == 'gaussian_filter'`, this may contain a 'sigma' entry,\n",
    "        which is the standard deviation of the Gaussian kernel.\n",
    "        A larger value yields a smoother but less close-fitting curve.\n",
    "        In the case of `fit_type == 'fourier'`, this may contain 'n_predict' or 'n_harm' entries.\n",
    "        The 'n_predict' entry is the number of points to extrapolate.\n",
    "        The points will be spaced evenly by the mean spacing of values in `x`.\n",
    "        The 'n_harm' entry is the number of harmonics to use.\n",
    "        A higher value yields a closer fit.\n",
    "    Returns\n",
    "    -------\n",
    "    x_smooth, y_smooth: numpy.ndarray\n",
    "        The smoothed x and y values of the curve fit.\n",
    "        If there are no non-NaN values in `y`, these will be filled with `n_pts` NaNs.\n",
    "        If there is only 1 non-NaN value in `y`, these will be filled with\n",
    "        their corresponding values (y or x value) for that point to a length of `n_pts`.\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    #from .curve_fitting import gaussian_fit, gaussian_filter_fit, poly_fit, fourier_fit\n",
    "\n",
    "    interpolation_curve_fits = ['gaussian', 'gaussian_filter',\n",
    "                                'poly', 'cubic_spline']\n",
    "    extrapolation_curve_filts = ['fourier']\n",
    "    # Handle NaNs (omit them).\n",
    "    not_nan_mask = ~np.isnan(y)\n",
    "    x = x[not_nan_mask]; y = y[not_nan_mask]\n",
    "\n",
    "    # Handle the cases of there being too few points to curve fit.\n",
    "    if len(y) == 0:\n",
    "        x_smooth = np.repeat(np.nan, n_pts)\n",
    "        y_smooth = np.repeat(np.nan, n_pts)\n",
    "        return x_smooth, y_smooth\n",
    "    if len(y) == 1:\n",
    "        x_smooth = np.repeat(x[0], n_pts)\n",
    "        y_smooth = np.repeat(y[0], n_pts)\n",
    "        return x_smooth, y_smooth\n",
    "\n",
    "    if x_smooth is None:\n",
    "        x_smooth_inds = np.linspace(0, len(x) - 1, n_pts)\n",
    "        x_smooth = np.interp(x_smooth_inds, np.arange(len(x)), x)\n",
    "\n",
    "    opt_params = {}\n",
    "    if fit_type == 'gaussian':\n",
    "        x_smooth, y_smooth = gaussian_fit(x, y, x_smooth)\n",
    "    elif fit_type == 'gaussian_filter':\n",
    "        if 'sigma' in fit_kwargs:\n",
    "            opt_params.update(dict(sigma=fit_kwargs.get('sigma')))\n",
    "        x_smooth, y_smooth = gaussian_filter_fit(x, y, x_smooth,\n",
    "                                                 **opt_params)\n",
    "    elif fit_type == 'poly':\n",
    "        assert 'degree' in fit_kwargs.keys(), \\\n",
    "            \"When plotting a polynomal fit, there must be\" \\\n",
    "            \"a 'degree' entry in the plot_kwargs parameter.\"\n",
    "        degree = fit_kwargs.get('degree')\n",
    "        x_smooth, y_smooth = poly_fit(x, y, degree, x_smooth)\n",
    "    elif fit_type == 'cubic_spline':\n",
    "        cs = CubicSpline(x, y)\n",
    "        y_smooth = cs(x_smooth)\n",
    "    if fit_type in extrapolation_curve_filts:\n",
    "        n_predict = fit_kwargs.get('n_predict', 0)\n",
    "        if fit_type == 'fourier':\n",
    "            if 'n_harm' in fit_kwargs:\n",
    "                opt_params.update(dict(n_harm=fit_kwargs.get('n_harm')))\n",
    "            x_smooth, y_smooth = \\\n",
    "                fourier_fit(x, y, n_predict, x_smooth,\n",
    "                            **opt_params)\n",
    "    return x_smooth, y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gaussian_filter_fit(x, y, x_smooth=None, n_pts=2000, sigma=0.75):\n",
    "    \"\"\"\n",
    "    Fits a Gaussian filter to some data - x and y. Returns predicted interpolation values.\n",
    "    Currently, smoothing is achieved by fitting a cubic spline to the gaussian filter fit\n",
    "    of `x` and `y`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: list-like\n",
    "        The x values of the data to fit to.\n",
    "    y: list-like\n",
    "        The y values of the data to fit to.\n",
    "    x_smooth: list-like, optional\n",
    "        The exact x values to interpolate for. Supercedes `n_pts`.\n",
    "    n_pts: int, optional\n",
    "        The number of evenly spaced points spanning the range of `x` to interpolate for.\n",
    "    sigma: numeric, optional\n",
    "        The standard deviation of the Gaussian kernel. A larger value yields a smoother curve,\n",
    "        but also reduced the closeness of the fit. By default, it is `0.75`.\n",
    "    Returns\n",
    "    -------\n",
    "    x_smooth, y_smooth: numpy.ndarray\n",
    "        The smoothed x and y values of the curve fit.\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "    if x_smooth is None:\n",
    "        x_smooth_inds = np.linspace(0, len(x)-1, n_pts)\n",
    "        x_smooth = np.interp(x_smooth_inds, np.arange(len(x)), x)\n",
    "    gauss_filter_y = gaussian_filter1d(y, sigma)\n",
    "    cs = CubicSpline(x, gauss_filter_y)\n",
    "    y_smooth = cs(x_smooth)\n",
    "    return x_smooth, y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_scale(arr, pop_arr=None, pop_min_max=None, pop_mean_std=None, min_max=None, scaling='norm'):\n",
    "    \"\"\"\n",
    "    Scales a NumPy array with standard scaling or norm scaling, default to norm scaling.\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: numpy.ndarray\n",
    "        The NumPy array to scale.\n",
    "    pop_arr: numpy.ndarray, optional\n",
    "        The NumPy array to treat as the population.\n",
    "        If specified, all members of `arr` must be within the range of `pop_arr`\n",
    "        or `min_max` must be specified.\n",
    "    pop_min_max: list-like, optional\n",
    "        The population minimum and maximum, in that order.\n",
    "        Supercedes `pop_arr` when normalizing.\n",
    "    pop_mean_std: list-like, optional\n",
    "        The population mean and standard deviation, in that order.\n",
    "        Supercedes `pop_arr` when standard scaling.\n",
    "    min_max: list-like, optional\n",
    "        The desired minimum and maximum of the final output, in that order.\n",
    "        If all values are the same, all values will become `min_max[0]`.\n",
    "    scaling: str, optional\n",
    "        The options are ['std', 'norm'].\n",
    "        The option 'std' standardizes. The option 'norm' normalizes (min-max scales).\n",
    "    \"\"\"\n",
    "    if len(arr) == 0:\n",
    "        return arr\n",
    "    pop_arr = arr if pop_arr is None else pop_arr\n",
    "    if scaling == 'norm':\n",
    "        pop_min, pop_max = (pop_min_max[0], pop_min_max[1]) if pop_min_max is not None \\\n",
    "            else (np.nanmin(pop_arr), np.nanmax(pop_arr))\n",
    "        numerator, denominator = arr - pop_min, pop_max - pop_min\n",
    "    elif scaling == 'std':\n",
    "        mean, std = pop_mean_std if pop_mean_std is not None else (np.nanmean(pop_arr), np.nanstd(pop_arr))\n",
    "        numerator, denominator = arr - mean, std\n",
    "    # Primary scaling\n",
    "    new_arr = arr\n",
    "    if denominator > 0:\n",
    "        new_arr = numerator / denominator\n",
    "    # Optional final scaling.\n",
    "    if min_max is not None:\n",
    "        if denominator > 0:\n",
    "            new_arr = np.interp(new_arr, (np.nanmin(new_arr), np.nanmax(new_arr)), min_max)\n",
    "        else: # The values are identical - set all values to the low end of the desired range.\n",
    "            new_arr = np.full_like(new_arr, min_max[0])\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_dt64_to_str(np_datetime, fmt='%Y-%m-%d'):\n",
    "    \"\"\"Converts a NumPy datetime64 object to a string based on a format string supplied to pandas strftime.\"\"\"\n",
    "    return pd.to_datetime(str(np_datetime)).strftime(fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a smoothed curve with a spread of the data in boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create NDVI Boxplot\n",
    "bin_by = 'month'\n",
    "\n",
    "aggregated_by_str = None\n",
    "if bin_by is None:\n",
    "    plotting_data = clean_dataset\n",
    "elif bin_by == 'week':\n",
    "    plotting_data = clean_dataset.resample(time='1w').mean()\n",
    "    aggregated_by_str = 'Week'\n",
    "elif bin_by == 'month':\n",
    "    plotting_data = clean_dataset.resample(time='1m').mean()\n",
    "    aggregated_by_str = 'Month'\n",
    "elif bin_by == 'weekofyear':\n",
    "    plotting_data = clean_dataset.groupby('time.week').mean(dim=('time'))\n",
    "    aggregated_by_str = 'Week of Year'\n",
    "elif bin_by == 'monthofyear':\n",
    "    plotting_data = clean_dataset.groupby('time.month').mean(dim=('time'))\n",
    "    aggregated_by_str = 'Month of Year'\n",
    "    \n",
    "params = dict(dataset=plotting_data, plot_descs={veg_proxy:{'none':[\n",
    "    {'box':{'boxprops':{'facecolor':'forestgreen'}}}]}})\n",
    "#change \n",
    "params['plot_descs'][veg_proxy][curve_fit_target] = [{'gaussian_filter':{}}]\n",
    "    \n",
    "fig, curve_fit_plotting_data = \\\n",
    "    xarray_time_series_plot(**params, fig_params=dict(figsize=(10,10), dpi=520), \n",
    "                            max_times_per_plot=max_times_per_plot)\n",
    "\n",
    "plt.title('Box-and-Whisker Plot of {1} with a Curvefit of {0} {1}'\n",
    "          .format(curve_fit_target.capitalize(), veg_proxy))\n",
    "plt.tight_layout()\n",
    "#plt.savefig('ndvi8_gauss') #saving to file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bin_by = 'monthofyear'\n",
    "years_with_data = []\n",
    "daysofyear_per_year = {}\n",
    "plotting_data_years = {}\n",
    "plot_descs = {}\n",
    "\n",
    "for year in range(np.datetime64(time_extents[0], 'Y').item().year,\n",
    "                  np.datetime64(time_extents[1], 'Y').item().year+1):\n",
    "    year_data = clean_dataset.sel(time=slice('{}-01-01'.format(year), '{}-12-31'.format(year)))[veg_proxy]\n",
    "    if len(year_data['time']) == 0: # There is nothing to plot for this year.\n",
    "        print(\"Year {} has no data, so will not be plotted.\".format(year))\n",
    "        continue\n",
    "    years_with_data.append(year)\n",
    "    spec_ind_dayofyear = year_data.groupby('time.dayofyear').mean()\n",
    "    daysofyear_per_year[year] = spec_ind_dayofyear[~spec_ind_dayofyear.isnull().sum(dim='dayofyear')].dayofyear\n",
    "    aggregated_by_str = None\n",
    "    if bin_by == 'weekofyear':\n",
    "        plotting_data_year = year_data.groupby('time.week').mean(dim=('time'))\n",
    "        time_dim_name = 'week'\n",
    "    elif bin_by == 'monthofyear':\n",
    "        plotting_data_year = year_data.groupby('time.month').mean(dim=('time'))\n",
    "        time_dim_name = 'month'\n",
    "\n",
    "    plotting_data_years[year] = plotting_data_year\n",
    "    num_time_pts = len(plotting_data_year[time_dim_name])\n",
    "    \n",
    "    # Select the curve-fit type. \n",
    "    # See the documentation for `xarray_time_series_plot()` regarding the `plot_descs` parameter.\n",
    "    plot_descs[year] = {'mean':[{'gaussian_filter':{}}]}\n",
    "    \n",
    "def retrieve_or_create_fig_ax(fig=None, ax=None, **subplots_kwargs):\n",
    "    \"\"\"\n",
    "    Returns appropriate matplotlib Figure and Axes objects given Figure and/or Axes objects.\n",
    "    If neither is supplied, a new figure will be created with associated axes.\n",
    "    If only `fig` is supplied, `(fig,fig.axes[0])` is returned. That is, the first Axes object will be used (and created if necessary).\n",
    "    If `ax` is supplied, `(fig, ax)` is returned.\n",
    "    Returns\n",
    "    -------\n",
    "    fig, ax: matplotlib.figure.Figure, matplotlib.axes.Axes\n",
    "        The figure and the axes of that figure.\n",
    "    **subplots_kwargs: dict\n",
    "        A dictionary of keyword arguments to passed to `matplotlib.pyplot.subplots()`,\n",
    "        such as `ncols` or `figsize`.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        if fig is None:\n",
    "            fig, ax = plt.subplots(**subplots_kwargs)\n",
    "        else:\n",
    "            if len(fig.axes) == 0:\n",
    "                fig.add_subplot(111)\n",
    "            ax = fig.axes[0]\n",
    "    return fig, ax\n",
    "\n",
    "def month_ints_to_month_names(month_ints):\n",
    "    \"\"\"\n",
    "    Converts ordinal numbers for months (in range [1,12]) to their 3-letter names.\n",
    "    \"\"\"\n",
    "    return [month_names_short[i - 1] for i in month_ints]\n",
    "\n",
    "month_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual seasonal curve calculation\n",
    "time_extents = ('2015-01-01', '2023-12-31')\n",
    "\n",
    "years_with_data = []\n",
    "plot_descs = {}\n",
    "daysofyear_per_year = {}\n",
    "plotting_data_years = {}\n",
    "time_dim_name = None\n",
    "for year in range(np.datetime64(time_extents[0], 'Y').item().year,\n",
    "                  np.datetime64(time_extents[1], 'Y').item().year+1):\n",
    "    year_data = clean_dataset.sel(time=slice('{}-01-01'.format(year), '{}-12-31'.format(year)))[veg_proxy]\n",
    "    if len(year_data['time']) == 0: # There is nothing to plot for this year.\n",
    "        print(\"Year {} has no data, so will not be plotted.\".format(year))\n",
    "        continue\n",
    "    years_with_data.append(year)\n",
    "    \n",
    "    spec_ind_dayofyear = year_data.groupby('time.dayofyear').mean()\n",
    "    daysofyear_per_year[year] = spec_ind_dayofyear[~spec_ind_dayofyear.isnull().sum(dim='dayofyear')].dayofyear\n",
    "    \n",
    "    aggregated_by_str = None\n",
    "    if bin_by == 'weekofyear':\n",
    "        plotting_data_year = year_data.groupby('time.week').mean(dim=('time'))\n",
    "        time_dim_name = 'week'\n",
    "    elif bin_by == 'monthofyear':\n",
    "        plotting_data_year = year_data.groupby('time.month').mean(dim=('time'))\n",
    "        time_dim_name = 'month'\n",
    "\n",
    "    plotting_data_years[year] = plotting_data_year\n",
    "    num_time_pts = len(plotting_data_year[time_dim_name])\n",
    "    \n",
    "    # Select the curve-fit type. \n",
    "    # See the documentation for `xarray_time_series_plot()` regarding the `plot_descs` parameter.\n",
    "    plot_descs[year] = {'mean':[{'gaussian_filter':{}}]}\n",
    "\n",
    "time_dim_name = 'week' if bin_by == 'weekofyear' else 'month' if bin_by == 'monthofyear' else 'time'\n",
    "\n",
    "num_times = 54 if bin_by == 'weekofyear' else 12\n",
    "time_coords_arr = np.arange(1, num_times+1) # In xarray, week and month indices start at 1.\n",
    "time_coords_da = xr.DataArray(time_coords_arr, coords={time_dim_name:time_coords_arr}, \n",
    "                              dims=[time_dim_name], name=time_dim_name)\n",
    "coords = dict(list(plotting_data_years.values())[0].coords)\n",
    "coords[time_dim_name] = time_coords_da \n",
    "plotting_data = xr.Dataset(plotting_data_years, coords=coords)\n",
    "params = dict(dataset=plotting_data, plot_descs=plot_descs)\n",
    "\n",
    "xarray_time_series_plot(**params, fig_params=dict(figsize=(9,6), dpi=150))\n",
    "plt.title('Line Plot of {0} for Each Year'.format(veg_proxy))\n",
    "plt.savefig('gauss_filter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual dataset comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median interpolation for years\n",
    "dataset_clean_year = clean_dataset.groupby('time.year').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# median interpolation for month\n",
    "dataset_clean_month = clean_dataset.groupby('time.month').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual difference between year 2022 (year 7) and 2017 (year 2)\n",
    "# 0=2015, 1=2016, 2=2017, 3=2018, 4=2019, 5=2020, 6=2021, 7=2022, 8=2023\n",
    "annual = (dataset_clean_year.ndvi.isel(year=7) - dataset_clean_year.ndvi.isel(year=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the NDVI difference\n",
    "annual.plot.imshow(size=10,cmap='Reds_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to geotiff\n",
    "# for tract 17020 for year 7, i.e. 2022\n",
    "write_cog(geo_im=dataset_clean_year.isel(year=7).ndvi,fname='17020_ndvi_2022.tif',overwrite=True,use_windowed_writes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a clustering using\n",
    "from skimage.segmentation import quickshift\n",
    "# Produce arrays\n",
    "input_array = annual.values.astype(np.float64)\n",
    "# Choose a year 0-7, i.e. 2015-2022\n",
    "# chose a band or index, e.g. ndvi8, red, ...\n",
    "input_array = dataset_clean_year.ndvi.isel(year=6).values.astype(np.float64)\n",
    "#\n",
    "# Calculate the segments\n",
    "segments = quickshift(input_array,\n",
    "                      kernel_size=2,\n",
    "                      convert2lab=False,\n",
    "                      max_dist=10,\n",
    "                      ratio=1.0)\n",
    "\n",
    "# Zonal mean calculation\n",
    "segments_zonal_mean_qs = scipy.ndimage.mean(input=input_array,\n",
    "                                            labels=segments,\n",
    "                                            index=segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at satistics\n",
    "print('Minimum value',input_array.min(),';','Maximum value',input_array.max(),';','Mean',input_array.mean(),';','Standard deviation',input_array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to see result\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(segments_zonal_mean_qs, cmap='RdYlGn')#, vmin=0, vmax=1)\n",
    "plt.colorbar(shrink=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to cog\n",
    "segments_zonal_mean_qs = xr.DataArray(segments_zonal_mean_qs, \n",
    "             coords=annual.coords, \n",
    "             dims=['y', 'x'], \n",
    "             attrs=annual.attrs)\n",
    "\n",
    "# Write array to GeoTIFF\n",
    "write_cog(geo_im=segments_zonal_mean_qs,\n",
    "          fname='masked_data_22.tif',\n",
    "          overwrite=True);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEA functions credits goes to\n",
    "# https://docs.digitalearthafrica.org/en/latest/sandbox/notebooks/Real_world_examples/Radar_urban_area_mapping.html\n",
    "def show_clustered(predicted_ds):\n",
    "    \"\"\"\n",
    "    Takes the predicted xarray DataArray and plots it.\n",
    "\n",
    "    Last modified: November 2021\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_ds : xarray DataArrray\n",
    "    The xarray DataArray which is the result of the k-means clustering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An plot of the predicted_ds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display predicted_ds Dataset with upto 6 unique classes.\n",
    "    image = predicted_ds\n",
    "\n",
    "    # Color list with 6 colors from the virdis color map.\n",
    "    no_classes = len(np.unique(image))\n",
    "    colour_list = [\"#fde725\", \"#440154\", \"#22a884\", \"#414487\", \"#DDA0DD\", \"#7ad151\",\"#7E1E9C\"]\n",
    "    colours = colour_list[:no_classes]\n",
    "    cmap = mcolours.ListedColormap(colours)\n",
    "    bounds = range(0, no_classes+1)\n",
    "    norm = mcolours.BoundaryNorm(np.array(bounds), cmap.N)\n",
    "    cblabels = [str(i) for i in bounds]\n",
    "    im = plt.imshow(image,cmap=cmap, norm=norm)\n",
    "    cb = plt.colorbar(im,cmap=cmap,norm=norm)\n",
    "    cb.set_ticks(np.linspace(1,3,3)-0.5)\n",
    "    cb.set_ticklabels(['1','2','3'])\n",
    "    #cb.set_ticklabels(cblabels)\n",
    "    plt.axis(\"off\")\n",
    "    #title = f\"K-means Clustering Predicted Image using {no_classes} clusters\"\n",
    "    #plt.title(title)\n",
    "    #plt.savefig(\"2016_6_clusters_dry.jpeg\", dpi=2000)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def kmeans_clustering(input_xr, cluster_range):\n",
    "    \"\"\"\n",
    "    Perform sklearn Kmeans clustering on the input Dataset\n",
    "    or Data Array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_xr : xarray.DataArray or xarray.Dataset\n",
    "        Must have dimensions 'x' and 'y', may have dimension 'time'.\n",
    "\n",
    "    cluster_range : list\n",
    "        A list of the number of clusters to use to perform the k-means clustering\n",
    "        on the input_xr Dataset.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    results : dictionary\n",
    "        A dictionary with the number of clusters as keys and the predicted xarray.DataArrays\n",
    "        as the values. Each predicted xarray.DataArray has the same dimensions 'x', 'y' and\n",
    "        'time' as the input_xr.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the sklearn_flatten function to convert the Dataset or DataArray into a 2 dimensional numpy array.\n",
    "    model_input = sklearn_flatten(input_xr)\n",
    "\n",
    "    # Standardize the data.\n",
    "    scaler = StandardScaler()\n",
    "    model_input = scaler.fit_transform(model_input)\n",
    "\n",
    "    # Dictionary to save results\n",
    "    results = {}\n",
    "\n",
    "    # Perform Kmeans clustering on the input dataset for each number of clusters\n",
    "    # in the cluster_range list.\n",
    "    for no_of_clusters in cluster_range:\n",
    "        # Set up the kmeans classification by specifying the number of clusters\n",
    "        # with initialization as k-means++.\n",
    "        km = KMeans(n_clusters=no_of_clusters, init=\"random\", random_state=1)\n",
    "\n",
    "        # Begin iteratively computing the position of the clusters.\n",
    "        km.fit(model_input)\n",
    "\n",
    "        # Use the sklearn kmeans .predict method to assign all the pixels of the\n",
    "        # model input to a unique cluster.\n",
    "        flat_predictions = km.predict(model_input)\n",
    "\n",
    "        # Use the sklearn_unflatten function to convert the flat predictions into a\n",
    "        # xarray DataArray.\n",
    "        predicted = sklearn_unflatten(flat_predictions, input_xr)\n",
    "        predicted = predicted.transpose(\"y\", \"x\")\n",
    "\n",
    "        # Append the results to a dictionary using the number of clusters as the\n",
    "        # column as an key.\n",
    "        results.update({str(no_of_clusters): predicted})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering first with 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input for clustering\n",
    "clusters = 2\n",
    "#\n",
    "median2022 = dataset_clean_year.sel(year=[2022])\n",
    "# change the input, e.g. from ndvi8 to red\n",
    "kmean_input = median2022.ndvi\n",
    "# Perform clustering\n",
    "ndvi_kmeans = kmeans_clustering(kmean_input,[clusters])\n",
    "# Depnding on the defined clusters will have discrete values\n",
    "for predicted_ds in ndvi_kmeans.values():\n",
    "    predicted_ds.plot.imshow(size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
